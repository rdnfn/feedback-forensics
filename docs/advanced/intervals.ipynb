{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33df951",
   "metadata": {},
   "source": [
    "# Confidence intervals for strength metric ðŸ”¬\n",
    "\n",
    "In this tutorial we show how to add confidence intervals to the *strength* metric via bootstrapping.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388f3b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“œ  | INFO | AnnotatedPairs format version: 2.0\u001b[0m\n",
      "ðŸ“œ  | INFO | Created 20000 annotations for 55 model annotators with 55 reference models in 0.29 seconds\u001b[0m\n",
      "ðŸ“œ  | INFO | Loaded data from path: ../../data/output/results_sets/feedback-forensics-results-paper/chatbot_arena.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "# Load results (e.g. Arena data)\n",
    "dataset_name = \"chatbot_arena.json\"\n",
    "dataset = ff.DatasetHandler()\n",
    "data_path = pathlib.Path(\"../../data/output/results_sets/feedback-forensics-results-paper\")\n",
    "dataset.add_data_from_path(data_path / dataset_name)\n",
    "df = dataset.first_handler.df\n",
    "\n",
    "annotator_metadata = dataset.get_available_annotators()\n",
    "metrics = dataset.get_annotator_metrics()\n",
    "\n",
    "# Get top and bottom 5 annotators according to strength metric\n",
    "strength_metrics = metrics[\"chatbot_arena\"][\"metrics\"][\"strength\"]\n",
    "annotators = list(strength_metrics.keys())\n",
    "top_annotators = sorted(annotators, key=lambda x: strength_metrics[x], reverse=True)\n",
    "top5_annotators = top_annotators[:5]\n",
    "bottom5_annotators = top_annotators[-5:][::-1]\n",
    "\n",
    "def get_annotator_key(in_row_name: str) -> str:\n",
    "    for annotator_key, metadata in annotator_metadata.items():\n",
    "        if metadata[\"annotator_in_row_name\"] in in_row_name:\n",
    "            return annotator_key\n",
    "    return None\n",
    "\n",
    "annotators = {\n",
    "    \"top5\": {\n",
    "        annotator_name: {\"key\": get_annotator_key(annotator_name), \"name\": annotator_name}\n",
    "        for annotator_name in top5_annotators\n",
    "    },\n",
    "    \"bottom5\": {\n",
    "        annotator_name: {\"key\": get_annotator_key(annotator_name), \"name\": annotator_name}\n",
    "        for annotator_name in bottom5_annotators\n",
    "    }\n",
    "}\n",
    "\n",
    "default_annotator_key = [key for key, info in annotator_metadata.items() if info[\"variant\"] == \"default_annotator\"][0]\n",
    "human_data = df[default_annotator_key]\n",
    "\n",
    "for category, annotator_subset in annotators.items():\n",
    "    for annotator_name in annotator_subset.keys():\n",
    "        annotator_key = annotator_subset[annotator_name][\"key\"]\n",
    "        annotator_data = df[annotator_key]\n",
    "        annotator_subset[annotator_name][\"data\"] = annotator_data\n",
    "\n",
    "        # create a combined dataset of human and annotator data\n",
    "        combined_data = []\n",
    "        assert len(human_data) == len(annotator_data), \"Human and annotator data have different lengths\"\n",
    "        for i in range(len(human_data)):\n",
    "            combined_data.append([human_data.iloc[i], annotator_data.iloc[i]])\n",
    "\n",
    "        annotator_subset[annotator_name][\"combined_data\"] = combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1854c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'is more verbose' from 'top5'\n",
      "Time taken for single metric: 0.02 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 1.67 seconds\n",
      "Processing 'has more structured formatting' from 'top5'\n",
      "Time taken for single metric: 0.02 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 1.29 seconds\n",
      "Processing 'makes more confident statements' from 'top5'\n",
      "Time taken for single metric: 0.02 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 1.14 seconds\n",
      "Processing 'is more factually correct' from 'top5'\n",
      "Time taken for single metric: 0.02 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 0.92 seconds\n",
      "Processing 'more strictly follows the requested output format' from 'top5'\n",
      "Time taken for single metric: 0.02 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 0.98 seconds\n",
      "Processing 'is more concise' from 'bottom5'\n",
      "Time taken for single metric: 0.02 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 1.67 seconds\n",
      "Processing 'has a more avoidant tone' from 'bottom5'\n",
      "Time taken for single metric: 0.01 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 0.34 seconds\n",
      "Processing 'refuses to answer the question' from 'bottom5'\n",
      "Time taken for single metric: 0.01 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 0.31 seconds\n",
      "Processing 'ends with a follow-up question' from 'bottom5'\n",
      "Time taken for single metric: 0.01 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 0.40 seconds\n",
      "Processing 'is more polite' from 'bottom5'\n",
      "Time taken for single metric: 0.01 seconds\n",
      "Starting bootstrap\n",
      "Time taken: 0.89 seconds\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def get_strength_metric(human_annotations, trait_annotations, axis=None):\n",
    "    \"\"\"Custom version of strength metric that is compatible with scipy bootstrapping.\n",
    "\n",
    "    Takes different input from the main metric implementation in ff.app.metrics.\n",
    "\n",
    "    Data is a list of tuples, where each tuple contains a human annotation and an trait annotation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create boolean mask for relevant annotations\n",
    "    relevant_mask = np.isin(trait_annotations, [\"text_a\", \"text_b\"])\n",
    "\n",
    "    # Get relevant trait annotations using mask\n",
    "    relevant_trait_annotations = np.array(trait_annotations)[relevant_mask]\n",
    "\n",
    "    relevance = len(relevant_trait_annotations) / len(trait_annotations)\n",
    "\n",
    "    # Get relevant human annotations using same mask\n",
    "    relevant_human_annotations = np.array(human_annotations)[relevant_mask]\n",
    "\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(\n",
    "        relevant_human_annotations,\n",
    "        relevant_trait_annotations,\n",
    "    )\n",
    "\n",
    "    return kappa * relevance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for category, annotator_subset in annotators.items():\n",
    "    for annotator_name, annotator_data in annotator_subset.items():\n",
    "        print(f\"Processing '{annotator_name}' from '{category}'\")\n",
    "        combined_data = annotator_data[\"combined_data\"][:10000]\n",
    "        human_annotations = [x[0] for x in combined_data]\n",
    "        trait_annotations = [x[1] for x in combined_data]\n",
    "        start_time = time.time()\n",
    "        annotator_data[\"strength_metric\"] = get_strength_metric(human_annotations, trait_annotations)\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for single metric: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Starting bootstrap\")\n",
    "        start_time = time.time()\n",
    "        annotator_data[\"strength_metric_confidence_interval\"] = scipy.stats.bootstrap(\n",
    "            (human_annotations, trait_annotations),\n",
    "            get_strength_metric,\n",
    "            confidence_level=0.95,\n",
    "            n_resamples=100,\n",
    "            vectorized=False,\n",
    "            paired=True,\n",
    "            axis=0,\n",
    "            method=\"percentile\",\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67d26de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator name | Strength | Low (CI 95%) | High (CI 95%)\n",
      "---|---|---|---\n",
      "is more verbose | 0.14 | 0.12 | 0.16\n",
      "has more structured formatting | 0.13 | 0.12 | 0.15\n",
      "makes more confident statements | 0.12 | 0.11 | 0.13\n",
      "is more factually correct | 0.11 | 0.10 | 0.12\n",
      "more strictly follows the requested output format | 0.09 | 0.07 | 0.10\n",
      "is more concise | -0.14 | -0.16 | -0.12\n",
      "has a more avoidant tone | -0.05 | -0.05 | -0.04\n",
      "refuses to answer the question | -0.04 | -0.05 | -0.04\n",
      "ends with a follow-up question | -0.01 | -0.02 | -0.00\n",
      "is more polite | -0.00 | -0.01 | 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Annotator name | Strength | Low (CI 95%) | High (CI 95%)\")\n",
    "print(\"---|---|---|---\")\n",
    "\n",
    "for category, annotator_subset in annotators.items():\n",
    "    for annotator_name, annotator_data in annotator_subset.items():\n",
    "        std_error = annotator_data['strength_metric_confidence_interval'].standard_error\n",
    "        cfdnc_interval = annotator_data['strength_metric_confidence_interval'].confidence_interval\n",
    "        print(f\"{annotator_name} | {annotator_data['strength_metric']:.2f} | {cfdnc_interval.low:.2f} | {cfdnc_interval.high:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b41dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
