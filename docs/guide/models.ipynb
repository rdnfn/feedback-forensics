{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Getting started:<br> Measure your model's personality ðŸ¤–\n",
    "\n",
    "Feedback Forensics can be used to measure your model's personality *relative* to other models. This tutorial will explain how in four steps (plus a bonus step):\n",
    "\n",
    "1. **Load data:** Load other models responses from HuggingFace (including prompts)\n",
    "2. **Generate responses:** Create responses to same prompts with *your model*\n",
    "3. **Annotate responses:** Combine your model's and other models' responses in single dataset and annotate that dataset\n",
    "4. **Personality analysis:** Analyse your model's personality with the app ðŸŽ‰\n",
    "5. **Bonus - Python analysis:** Analyse your model's personality using Python API\n",
    "\n",
    "```{important}\n",
    "To run all cells, this tutorial requires the `OPENROUTER_API_KEY` variable to be set.\n",
    "```\n",
    "\n",
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "prompts = datasets.load_dataset(\"rdnfn/ff-model-personality\", split=\"prompts_v1\")[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate your model's responses\n",
    "\n",
    "Next we generate your model's responses on the same prompts. Overall there are 500 unique prompts in this dataset.\n",
    "\n",
    "```{note}\n",
    "The sample size in this tutorial is intentionally tiny by default to keep cost low. Change the `NUM_PROMPTS` variable below to increase the sample size.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "scroll-output",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from feedback_forensics.tools.ff_modelgen import run_model_on_prompts_async\n",
    "import random\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "#Â If not set elsewhere, set openrouter api key\n",
    "# import os\n",
    "# os.environ[\"OPENROUTER_API_KEY\"] = \"...\"\n",
    "\n",
    "NUM_PROMPTS = 10 # to keep this example cheap, increase for representative results\n",
    "random.seed(42)\n",
    "prompts = random.sample(prompts, NUM_PROMPTS)\n",
    "\n",
    "# This will generate a jsonl file with an API model's responses\n",
    "# Replace the model name with another API model or generate your\n",
    "# responses separately with your own endpoint.\n",
    "\n",
    "model = \"openrouter/openai/gpt-4o-mini\"\n",
    "output_path = pathlib.Path(\"tmp/model_responses/\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "await run_model_on_prompts_async(\n",
    "    prompts=prompts,\n",
    "    model_name=model,\n",
    "    output_path=output_path,\n",
    "    max_concurrent=30, # Adjust based on your needs\n",
    "    max_tokens=4096,\n",
    ")\n",
    "\n",
    "responses_files =  output_path / \"generations\" / (model + \".jsonl\")\n",
    "generations = pd.read_json(responses_files, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "### Merge generations into pairwise dataset to annotate\n",
    "\n",
    "data = []\n",
    "\n",
    "# get gpt-4o generations to compare against\n",
    "gpt4o_name = \"openai/gpt-4o-2024-08-06\"\n",
    "prompt_df = df[df[\"prompt\"].isin(prompts)].copy()\n",
    "prompt_df[\"responses\"] = prompt_df.apply(lambda x: [x[\"response_a\"], x[\"response_b\"]], axis=1)\n",
    "prompt_df[\"gpt4o_response\"] = prompt_df[\"responses\"].apply(lambda x: x[0][\"text\"] if x[0][\"model\"] == gpt4o_name else x[1][\"text\"] if x[1][\"model\"] == gpt4o_name else None)\n",
    "\n",
    "# compile responses\n",
    "for prompt in prompts:\n",
    "    # get your model generations\n",
    "    model_response = generations[generations[\"prompt\"] == prompt].iloc[0][\"response\"]\n",
    "    gpt4o_response = prompt_df[prompt_df[\"prompt\"] == prompt][\"gpt4o_response\"].iloc[0]\n",
    "\n",
    "    data.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"text_a\": model_response,\n",
    "        \"text_b\": gpt4o_response,\n",
    "        \"model_a\": model,\n",
    "        \"model_b\": gpt4o_name\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(data)\n",
    "comparison_df.to_csv(\"tmp/model_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotating the data\n",
    "\n",
    "Then we use the `ff-annotate` CLI to collect personality annotations for our dataset. The annotated results will be saved to the `annotated_model_data/` dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "scroll-output",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!ff-annotate --datapath tmp/model_comparison.csv --output-dir tmp/annotated_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrative screenshot of the visualisation ([see here](https://app.feedbackforensics.com/?data=chatbot_arena&ann_cols=model_gpt4o20240513,model_claude35sonnet20240620,model_gemini15proapi0514,model_mistrallarge2407,model_deepseekv2api0628) for an online example of a visualised result comparing models):\n",
    "\n",
    "```{figure}  ../img/example11_model_comparison_app.png\n",
    ":alt: screenshot\n",
    ":width: 400px\n",
    ":align: center\n",
    "---\n",
    "name: screenshot\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualising model personality with app\n",
    "\n",
    "Finally, we use the Feedback Forensics App to visualise the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "feedback-forensics -d tmp/annotated_model_data/results/070_annotations_train_ap.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Computing metrics in Python (Optional)\n",
    "\n",
    "Now, we have additional *personality* annotations. Next, we use the Feedback Forensics Python API to compute personality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import feedback_forensics as ff\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "dataset = ff.DatasetHandler()\n",
    "dataset.add_data_from_path(\"tmp/annotated_model_data/results/070_annotations_train_ap.json\")\n",
    "\n",
    "# set annotators to two models we consider here\n",
    "model_annotators = [ann for ann in dataset.get_available_annotator_visible_names() if \"Model\" in ann]\n",
    "dataset.set_annotator_cols(annotator_visible_names=model_annotators)\n",
    "\n",
    "# compute metrics\n",
    "annotator_metrics = dataset.get_annotator_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get strength of each personality trait\n",
    "kappa = pd.Series(annotator_metrics[f\"Model: {model.replace('openrouter/','')}\"][\"metrics\"][\"strength\"])\n",
    "\n",
    "# Get top 5 personality traits (by strength)\n",
    "print(f\"\\n## Top 5 personality traits in {model} (by strength):\\n{kappa.sort_values(ascending=False).head(5)}\")\n",
    "\n",
    "print(f\"\\n## Bottom 5 personality traits in {model} (by strength):\\n{kappa.sort_values(ascending=True).head(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
