{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dca987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up prompts from MultiPref\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "multipref_path = pathlib.Path(\"../data/input/allenai/multipref_original.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(multipref_path)\n",
    "print(f\"Number of unique data sources: {len(df['source'].unique())}\")\n",
    "\n",
    "# filter out rows from anthropic helpful/harmless\n",
    "df = df[~df[\"source\"].isin([\"anthropic/helpful-base\", \"anthropic/harmless-base\"])]\n",
    "\n",
    "prompts = df[\"text\"].unique()\n",
    "\n",
    "num_prompts = len(prompts)\n",
    "print(f\"Number of prompts: {num_prompts}\")\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 4321\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Shuffle the prompts\n",
    "shuffled_prompts = list(prompts)\n",
    "random.shuffle(shuffled_prompts)\n",
    "\n",
    "# save shuffled prompts to json\n",
    "# Save the shuffled prompts to a JSON file\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"../data/output\", exist_ok=True)\n",
    "\n",
    "# Save the shuffled prompts to a JSON file\n",
    "output_path = pathlib.Path(\"../data/input/allenai/multipref_shuffled_prompts.json\")\n",
    "if not output_path.exists():\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(shuffled_prompts, f, indent=2)\n",
    "else:\n",
    "    # Load the shuffled prompts from the JSON file\n",
    "    with open(output_path, \"r\") as f:\n",
    "        shuffled_prompts = json.load(f)\n",
    "\n",
    "\n",
    "chosen_prompts = shuffled_prompts[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_forensics.tools.model_comparison import run_model_on_prompts_async\n",
    "\n",
    "output_path = pathlib.Path(\"../data/output/model_comparison_v2/\")\n",
    "# not using all models to limit costs\n",
    "model_list = [\n",
    "    #\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    #\"openai/gpt-4o-2024-11-20\",\n",
    "    \"openai/gpt-4o-2024-08-06\", # current default version\n",
    "    #\"openai/gpt-4o-2024-05-13\",\n",
    "    #\"openai/chatgpt-4o-latest\", # unfortuantely no pinned version available\n",
    "    \"openai/gpt-4.1-2025-04-14\",\n",
    "    #\"openai/gpt-4.1-mini-2025-04-14\",\n",
    "    \"openai/gpt-3.5-turbo\",\n",
    "    \"openrouter/mistralai/mistral-medium-3\",\n",
    "    \"openrouter/mistralai/mistral-medium\",\n",
    "    \"openrouter/mistralai/mistral-7b-instruct-v0.1\",\n",
    "    \"openrouter/meta-llama/llama-4-maverick\",\n",
    "    \"openrouter/meta-llama/llama-3.3-70b-instruct\",\n",
    "    \"openrouter/meta-llama/llama-3-70b-instruct\",\n",
    "    \"openrouter/meta-llama/llama-2-70b-chat\",\n",
    "]\n",
    "\n",
    "print(f\"Running {len(model_list)} models\")\n",
    "\n",
    "for model in model_list:\n",
    "    print(f\"Running '{model}'\")\n",
    "    await run_model_on_prompts_async(\n",
    "        prompts=chosen_prompts,\n",
    "        model_name=model,\n",
    "        output_path=output_path,\n",
    "        max_concurrent=30, # Adjust based on your needs\n",
    "        max_tokens=4096, # max of gpt-3.5-turbo and gpt-4o-2024-05-13\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a519ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all generations\n",
    "generations = []\n",
    "for file in output_path.glob(\"**/generations/**/*.jsonl\"):\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                generations.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line in {file}: {e}\")\n",
    "\n",
    "generations = pd.DataFrame(generations)\n",
    "generations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad536fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = \"openai/gpt-4o-2024-08-06\" # default version of gpt-4o as of 11.05.2025\n",
    "\n",
    "# flipping seed (random_seed)\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# create a new table with the reference model\n",
    "comparison_data = []\n",
    "\n",
    "for model in model_list:\n",
    "    if model == reference_model:\n",
    "        continue\n",
    "\n",
    "    # Get all prompts that have both model and reference model responses\n",
    "    model_responses = generations[generations['model'] == model]\n",
    "    ref_responses = generations[generations['model'] == reference_model]\n",
    "\n",
    "    # Find common prompt_ids\n",
    "    common_prompt_ids = set(model_responses['prompt_id']).intersection(set(ref_responses['prompt_id']))\n",
    "\n",
    "    for prompt_id in common_prompt_ids:\n",
    "        model_row = model_responses[model_responses['prompt_id'] == prompt_id].iloc[0]\n",
    "        ref_row = ref_responses[ref_responses['prompt_id'] == prompt_id].iloc[0]\n",
    "\n",
    "        flip_order = np.random.rand() < 0.5\n",
    "\n",
    "        if flip_order:\n",
    "            text_a = ref_row['response']\n",
    "            text_b = model_row['response']\n",
    "            model_a = reference_model\n",
    "            model_b = model\n",
    "        else:\n",
    "            text_a = model_row['response']\n",
    "            text_b = ref_row['response']\n",
    "            model_a = model\n",
    "            model_b = reference_model\n",
    "\n",
    "        comparison_data.append({\n",
    "            'prompt_id': prompt_id,\n",
    "            'prompt': model_row['prompt'],\n",
    "            'text_a': text_a,\n",
    "            'text_b': text_b,\n",
    "            'model_a': model_a,\n",
    "            'model_b': model_b\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Shuffle order of comparison_df\n",
    "comparison_df = comparison_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Randomly flip the order of the models, text_a and text_b\n",
    "\n",
    "print(f\"Created comparison dataset with {len(comparison_df)} pairs\")\n",
    "\n",
    "comparison_df.to_csv(\"../data/output/model_comparison_v2/comparison_data_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3381e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icai-exp data_path=\"../data/output/model_comparison_v2/comparison_data_v3.csv\" s0_added_standard_principles_to_test=\"[v4]\" annotator.skip=true s0_skip_principle_generation=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b722ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
