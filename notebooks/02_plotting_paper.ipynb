{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedback_forensics as ff\n",
    "import feedback_forensics.app.plotting.paper as paper_plot\n",
    "import pathlib\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "data_path = pathlib.Path(\"../forensics-data/feedback-forensics-public-results\")\n",
    "fig_save_path = pathlib.Path(\"./output/png\")\n",
    "tex_save_path = pathlib.Path(\"./output/tex\")\n",
    "\n",
    "# ensure save path exists\n",
    "fig_save_path.mkdir(parents=True, exist_ok=True)\n",
    "tex_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# save general latex preamble\n",
    "with open(tex_save_path / \"000_preamble.tex\", \"w\") as f:\n",
    "    f.write(paper_plot.get_latex_doc_preamble())\n",
    "\n",
    "# example latex table\n",
    "with open(tex_save_path / \"999_example_table.tex\", \"w\") as f:\n",
    "    latex = []\n",
    "    latex = paper_plot.add_table_preamble(latex, \"Example Table\")\n",
    "    latex = paper_plot.add_table_postamble(latex)\n",
    "    f.write(\"\\n\".join(latex))\n",
    "\n",
    "datasets = [\n",
    "    [\"multipref_10k_v3.json\", \"MultiPref\"],\n",
    "    [\"llama4_arena_vs_public_version.json\", \"Llama 4 Arena vs Public\"],\n",
    "    [\"arena\", \"Chatbot Arena\"],\n",
    "    [\"prism\", \"PRISM\"],\n",
    "]\n",
    "\n",
    "cache = {}\n",
    "for dataset_path, dataset_name in datasets:\n",
    "    dataset = ff.DatasetHandler(cache=cache)\n",
    "    dataset.add_data_from_path(data_path / dataset_path)\n",
    "\n",
    "    overall_metrics = dataset.get_overall_metrics()\n",
    "    annotator_metrics = dataset.get_annotator_metrics()\n",
    "\n",
    "    metric_name = \"strength\"\n",
    "    strength_metrics = annotator_metrics[dataset_path.split(\".\")[0]][\"metrics\"][metric_name]\n",
    "\n",
    "    kwargs = {}\n",
    "    if dataset_path == \"llama4_arena_vs_public_version.json\":\n",
    "        kwargs = {\n",
    "            \"top_title\": \"Traits stronger in arena relative to public model\",\n",
    "            \"bottom_title\": \"Traits weaker in arena relative to public model\",\n",
    "        }\n",
    "\n",
    "    latex_table = paper_plot.get_latex_top_and_bottom_annotators(\n",
    "        annotator_metrics=strength_metrics,\n",
    "        metric_name=metric_name.capitalize(),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with open(tex_save_path / f\"001_top_and_bottom_annotators_{dataset_path.split('.')[0]}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Arena data\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"arena\"\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(data_path / dataset_name)\n",
    "\n",
    "general_df = dataset.first_handler.df\n",
    "values = [\n",
    "    'Creative Writing Prompts',\n",
    "    'Songwriting Prompts',\n",
    "    'Resume and Cover Letter Writing',\n",
    "    'Professional Email Communication',\n",
    "]\n",
    "dataset.split_by_col(col=\"narrower_category\", selected_vals=values)\n",
    "\n",
    "metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=metrics_df.head(10),\n",
    "    title=\"Encouraged personality traits across writing domains in Chatbot Arena (Strength)\",\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"002_writing_tasks_arena.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of MultiPref data\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "cache = {}\n",
    "data_path = pathlib.Path(\"../forensics-data/feedback-forensics-public-results\")\n",
    "\n",
    "dataset_name = \"multipref_10k_v3.json\"\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_metadata = dataset.get_available_annotators()\n",
    "special_annotators = {\n",
    "    annotator_key: metadata\n",
    "    for annotator_key, metadata in annotator_metadata.items()\n",
    "    if metadata[\"variant\"] in [\"unknown\", \"human\"]\n",
    "    if \"normal\" in metadata[\"annotator_visible_name\"] or \"expert\" in metadata[\"annotator_visible_name\"] or \"gpt4\" in metadata[\"annotator_visible_name\"]\n",
    "}\n",
    "special_annotators\n",
    "\n",
    "dataset.set_annotator_cols(annotator_keys=list(special_annotators.keys()))\n",
    "df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'multipref_10k_v3\\n(unknown: expert_1_preferred_text)': 'Human Expert 2',\n",
    "    'multipref_10k_v3\\n(unknown: preferred_text_gpt4)': 'GPT-4',\n",
    "    'multipref_10k_v3\\n(unknown: normal_0_preferred_text)': 'Human Regular 1',\n",
    "    'multipref_10k_v3\\n(unknown: normal_1_preferred_text)': 'Human Regular 2',\n",
    "    'multipref_10k_v3\\n(unknown: expert_0_preferred_text)': 'Human Expert 1',\n",
    "}\n",
    "# rename the columns\n",
    "df.rename(rename_dict, inplace=True, axis=1)\n",
    "\n",
    "# reorder the columns (experts, regular, gpt-4)\n",
    "df = df[['Generate a response that...', 'Human Expert 1', 'Human Expert 2', 'Human Regular 1', 'Human Regular 2', 'GPT-4', 'Max diff']]\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=df.head(5),\n",
    "    title=\"Personality traits encouraged by different annotators on MultiPref (Strength)\",\n",
    "    first_col_width=0.15,\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"003_cross_annotator_comparison_multipref.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model analysis\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "results_path = pathlib.Path(\"exp/outputs/2025-05-12_14-46-21_mc_v1/results/070_annotations_train_ap.json\")\n",
    "\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(results_path)\n",
    "\n",
    "annotators = dataset.get_available_annotators()\n",
    "model_anns = {\n",
    "    k: v for k, v in annotators.items() if v[\"variant\"] == \"model_identity\"\n",
    "}\n",
    "\n",
    "dataset.set_annotator_cols(annotator_keys=list(model_anns.keys()))\n",
    "metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "# remove the 070_annotations_train_ap from each column name\n",
    "metrics_df.columns = metrics_df.columns.str.replace(\"070_annotations_train_ap\\n(Model: \", \"\").str.replace(\")\", \"\").str.replace(\"openrouter/\", \"\").str.replace(\" \", \"-\")\n",
    "\n",
    "# rename max-diff to Max diff\n",
    "metrics_df.rename(columns={\"Max-diff\": \"Max diff\"}, inplace=True)\n",
    "\n",
    "# set all gpt-4o models to 0\n",
    "metrics_df[\"openai/gpt-4o-2024-08-06\"] = 0\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=metrics_df.head(5),\n",
    "    title=\"Most diverging personality traits across models\",\n",
    "    first_col_width=0.15,\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"004_model_comparison.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(metrics_df.head(5).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "short_names = {\n",
    "    'meta-llama/llama-3-70b-instruct': \"Llama-3-70b\",\n",
    "    'meta-llama/llama-3.3-70b-instruct':  \"Llama-3.3-70b\",\n",
    "    'meta-llama/llama-4-maverick': \"Llama-4-Maverick\",\n",
    "    'mistralai/mistral-medium': \"Mistral-Medium\",\n",
    "    'mistralai/mistral-medium-3': \"Mistral-Medium-3\",\n",
    "    'openai/gpt-4.1-2025-04-14': \"GPT-4.1\",\n",
    "    'openai/gpt-4o-2024-08-06': \"GPT-4o\",\n",
    "    'meta-llama/llama-2-70b-chat': \"Llama-2-70b\",\n",
    "    'openai/gpt-3.5-turbo': \"GPT-3.5-Turbo\",\n",
    "    'mistralai/mistral-7b-instruct-v0.1': \"Mistral-7b\",\n",
    "}\n",
    "\n",
    "\n",
    "def plot_model_comparison_by_family(metrics_df, trait_index=\"makes more confident statements\", ax=None):\n",
    "    # Import adjustText for automatic label positioning\n",
    "    from adjustText import adjust_text\n",
    "\n",
    "    # Create axis if not provided\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(3.5, 2.5))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    # Extract data for the plot\n",
    "    data = metrics_df.loc[trait_index].drop(\"Max diff\")\n",
    "    # Group models by family\n",
    "    model_families = {\n",
    "        \"Meta\": sorted([col for col in data.index if \"llama\" in col.lower()]),\n",
    "        \"Mistral\": sorted([col for col in data.index if \"mistral\" in col.lower()]),\n",
    "        \"OpenAI\": sorted([col for col in data.index if \"gpt\" in col.lower()],\n",
    "                  key=lambda x: 0 if \"gpt-4o\" in x.lower() else 1 if \"gpt-4.1\" in x.lower() else -1)\n",
    "    }\n",
    "\n",
    "    # Plot each model family with a different color\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green\n",
    "    markers = ['o', 's', '^']\n",
    "\n",
    "    # Store text objects for adjustText and lines for objects to avoid\n",
    "    texts = []\n",
    "    line_objects = []\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "\n",
    "    # Calculate x positions for each family\n",
    "    family_centers = []\n",
    "    family_widths = []\n",
    "\n",
    "    # First pass to determine x positions\n",
    "    start_x = 0\n",
    "    for i, (family, models) in enumerate(model_families.items()):\n",
    "        if len(models) > 1:\n",
    "            width = len(models)\n",
    "        else:\n",
    "            width = 1\n",
    "\n",
    "        family_widths.append(width)\n",
    "        family_centers.append(start_x + width/2)\n",
    "        start_x += width + 1  # Add spacing between families\n",
    "\n",
    "    for i, (family, models) in enumerate(model_families.items()):\n",
    "        family_data = data[models]\n",
    "        # Calculate x positions for this family\n",
    "        if len(models) > 1:\n",
    "            x = np.linspace(family_centers[i] - family_widths[i]/2 + 0.5,\n",
    "                           family_centers[i] + family_widths[i]/2 - 0.5,\n",
    "                           len(models))\n",
    "        else:\n",
    "            x = np.array([family_centers[i]])\n",
    "\n",
    "        line, = ax.plot(x, family_data.values, marker=markers[i], linestyle='-',\n",
    "                 color=colors[i], linewidth=2, markersize=8)\n",
    "        line_objects.append(line)\n",
    "\n",
    "        # Add model names as labels\n",
    "        for j, model in enumerate(models):\n",
    "            label = short_names[model]\n",
    "            text_obj = ax.text(x[j], family_data.values[j],\n",
    "                     label,\n",
    "                     ha='center', va='center', fontsize=8)\n",
    "            texts.append(text_obj)\n",
    "\n",
    "            x_coords.append(x[j])\n",
    "            y_coords.append(family_data.values[j])\n",
    "\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7, axis='y')\n",
    "    ax.set_ylabel(f'Strength of trait')\n",
    "\n",
    "    # Set x-ticks at the center of each family group with family names\n",
    "    ax.set_xticks(family_centers)\n",
    "    ax.set_xticklabels(model_families.keys())\n",
    "    ax.tick_params(axis='x', which='both', length=0)  # Make tick marks invisible but keep labels\n",
    "\n",
    "    ax.set_yticks([], minor=True)\n",
    "    ax.set_title(f'{trait_index.capitalize()}')\n",
    "\n",
    "    # No legend needed as we're using x-axis labels\n",
    "\n",
    "    # Use adjustText to automatically position labels without overlap\n",
    "    adjust_text(texts,\n",
    "                objects=line_objects,\n",
    "                arrowprops=dict(arrowstyle='-', color='gray', lw=0.5),\n",
    "                ax=ax,\n",
    "                expand=(1.2, 1.2),\n",
    "                avoid_self=True,\n",
    "                force_explode=(0.5, 0.5)\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_multiple_traits_side_by_side(metrics_df, traits, ncols=2):\n",
    "    \"\"\"\n",
    "    Plot multiple trait comparisons side by side\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics_df : DataFrame\n",
    "        The metrics dataframe\n",
    "    traits : list\n",
    "        List of traits to plot\n",
    "    ncols : int\n",
    "        Number of columns in the grid layout\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fig : Figure\n",
    "        The matplotlib figure\n",
    "    \"\"\"\n",
    "    nrows = (len(traits) + ncols - 1) // ncols  # Calculate number of rows needed\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3.5 * ncols, 2 * nrows))\n",
    "\n",
    "    # Make axes iterable even if there's only one subplot\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each trait\n",
    "    for i, trait in enumerate(traits):\n",
    "        if i < len(axes):\n",
    "            plot_model_comparison_by_family(metrics_df, trait, ax=axes[i])\n",
    "            # Remove y label on the second plot in each row\n",
    "            if i % ncols != 0:\n",
    "                axes[i].set_ylabel('')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(len(traits), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage - plot for confidence\n",
    "traits = [\n",
    "    \"makes more confident statements\",\n",
    "    \"provides a numbered list format\",\n",
    "    \"has a friendlier tone\",\n",
    "    \"ends with a follow-up question\",\n",
    "    \"expresses more emotion\",\n",
    "]\n",
    "\n",
    "comparison_path = fig_save_path / \"model_comparison\"\n",
    "comparison_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for trait in traits:\n",
    "    fig = plot_model_comparison_by_family(metrics_df, trait)\n",
    "    fig.savefig(comparison_path / (trait.replace(\" \", \"_\") + \".png\"), dpi=300)\n",
    "\n",
    "\n",
    "# Create double figures with two traits side by side\n",
    "for i in range(0, len(traits), 2):\n",
    "    if i + 1 < len(traits):  # Make sure we have a pair\n",
    "        trait_group = [traits[i], traits[i+1]]\n",
    "        fig = plot_multiple_traits_side_by_side(metrics_df, trait_group, ncols=2)\n",
    "        fig.savefig(comparison_path / f\"double_figure_{i//2}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
