{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedback_forensics as ff\n",
    "import feedback_forensics.app.plotting.paper as paper_plot\n",
    "import pathlib\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "data_path = pathlib.Path(\"../feedback-forensics-results\")\n",
    "fig_save_path = pathlib.Path(\"./output/png\")\n",
    "tex_save_path = pathlib.Path(\"./output/tex\")\n",
    "\n",
    "# ensure save path exists\n",
    "fig_save_path.mkdir(parents=True, exist_ok=True)\n",
    "tex_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tex_app_save_path = pathlib.Path(\"./output/tex/appendix\")\n",
    "tex_app_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# save general latex preamble\n",
    "with open(tex_save_path / \"000_preamble.tex\", \"w\") as f:\n",
    "    f.write(paper_plot.get_latex_doc_preamble())\n",
    "\n",
    "# example latex table\n",
    "with open(tex_save_path / \"999_example_table.tex\", \"w\") as f:\n",
    "    latex = []\n",
    "    latex = paper_plot.add_table_preamble(latex, \"Example Table\")\n",
    "    latex = paper_plot.add_table_postamble(latex)\n",
    "    f.write(\"\\n\".join(latex))\n",
    "\n",
    "main_pref_datasets = [\n",
    "    [data_path / \"allenai_multipref.json\", \"MultiPref\"],\n",
    "    [data_path / \"chatbot_arena.json\", \"Chatbot Arena\"],\n",
    "    [data_path / \"prism.json\", \"PRISM\"],\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    [pathlib.Path(\"../forensics-data/feedback-forensics-public-results/llama4_arena_vs_public_version.json\"), \"Llama 4 Arena vs Public\"],\n",
    "    *main_pref_datasets,\n",
    "]\n",
    "\n",
    "print(str(data_path / \"allenai_multipref.json\"))\n",
    "\n",
    "cache = {}\n",
    "for dataset_path, dataset_name in datasets:\n",
    "    dataset = ff.DatasetHandler(cache=cache)\n",
    "    dataset.add_data_from_path(dataset_path)\n",
    "\n",
    "    overall_metrics = dataset.get_overall_metrics()\n",
    "    annotator_metrics = dataset.get_annotator_metrics()\n",
    "\n",
    "    dataset_key = dataset_path.name.split(\".\")[0]\n",
    "\n",
    "    metric_name = \"strength\"\n",
    "    strength_metrics = annotator_metrics[dataset_key][\"metrics\"][metric_name]\n",
    "\n",
    "    kwargs = {}\n",
    "    if dataset_name == \"Llama 4 Arena vs Public\":\n",
    "        kwargs = {\n",
    "            \"top_title\": \"Traits stronger in arena relative to public model\",\n",
    "            \"bottom_title\": \"Traits weaker in arena relative to public model\",\n",
    "        }\n",
    "    else:\n",
    "        kwargs = {\n",
    "            \"top_title\": \"Ten most encouraged personality traits\",\n",
    "            \"bottom_title\": \"Ten least encouraged personality traits\",\n",
    "        }\n",
    "\n",
    "    latex_table = paper_plot.get_latex_top_and_bottom_annotators(\n",
    "        annotator_metrics=strength_metrics,\n",
    "        metric_name=metric_name.capitalize(),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with open(tex_save_path / f\"001_top_and_bottom_annotators_{dataset_key}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "    #Â appendix\n",
    "\n",
    "    latex_table_app = paper_plot.get_latex_top_and_bottom_annotators(\n",
    "        annotator_metrics=strength_metrics,\n",
    "        metric_name=metric_name.capitalize(),\n",
    "        top_n=10,\n",
    "        bottom_n=10,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with open(tex_app_save_path / f\"001_top_and_bottom_annotators_{dataset_key}.tex\", \"w\") as f:\n",
    "        f.write(latex_table_app)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of preference datasets relative to each other\n",
    "\n",
    "combined_dataset = ff.DatasetHandler(cache=cache)\n",
    "combined_dataset.load_data_from_paths([str(dataset[0]) for dataset in main_pref_datasets])\n",
    "\n",
    "pretty_names = {\n",
    "    \"allenai_multipref.json\": \"MultiPref\",\n",
    "    \"chatbot_arena.json\": \"Chatbot Arena\",\n",
    "    \"prism.json\": \"PRISM\",\n",
    "}\n",
    "\n",
    "for i, metric in enumerate([\"strength\", \"relevance\", \"cohens_kappa_randomized\"]):\n",
    "    for length in [5,20,40]:\n",
    "        metrics_df = combined_dataset.get_annotator_metrics_df(metric_name=metric, index_col_name=\"Generate a response that...\")\n",
    "        metrics_df.rename(columns=pretty_names, inplace=True)\n",
    "        latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "            metrics_df=metrics_df.head(length),\n",
    "            title=f\"Differences between datasets (metric: {metric})\",\n",
    "            first_col_width=0.3,\n",
    "        )\n",
    "\n",
    "        with open(tex_app_save_path / f\"00{i}_cross_datasets_{metric}_top{length}.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Arena data\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"chatbot_arena.json\"\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(data_path / dataset_name)\n",
    "\n",
    "general_df = dataset.first_handler.df\n",
    "values = [\n",
    "    'Songwriting Prompts',\n",
    "    'Resume and Cover Letter Writing',\n",
    "    'Professional Email Communication',\n",
    "]\n",
    "dataset.split_by_col(col=\"narrower_category\", selected_vals=values)\n",
    "\n",
    "metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=metrics_df.head(5),\n",
    "    title=\"Encouraged personality traits across writing domains in Chatbot Arena (Strength)\",\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"002_writing_tasks_arena.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of MultiPref data\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "cache = {}\n",
    "data_path = pathlib.Path(\"../feedback-forensics-results\")\n",
    "\n",
    "dataset_name = \"allenai_multipref.json\"\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_metadata = dataset.get_available_annotators()\n",
    "special_annotators = {\n",
    "    annotator_key: metadata\n",
    "    for annotator_key, metadata in annotator_metadata.items()\n",
    "    if metadata[\"variant\"] in [\"unknown\", \"human\"]\n",
    "    if \"normal\" in metadata[\"annotator_visible_name\"] or \"expert\" in metadata[\"annotator_visible_name\"] or \"gpt4\" in metadata[\"annotator_visible_name\"]\n",
    "}\n",
    "special_annotators\n",
    "\n",
    "dataset.set_annotator_cols(annotator_keys=list(special_annotators.keys()))\n",
    "df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'allenai_multipref\\n(unknown: expert_1_preferred_text)': 'Human Expert 2',\n",
    "    'allenai_multipref\\n(unknown: preferred_text_gpt4)': 'GPT-4-Turbo',\n",
    "    'allenai_multipref\\n(unknown: normal_0_preferred_text)': 'Human Regular 1',\n",
    "    'allenai_multipref\\n(unknown: normal_1_preferred_text)': 'Human Regular 2',\n",
    "    'allenai_multipref\\n(unknown: expert_0_preferred_text)': 'Human Expert 1',\n",
    "}\n",
    "# rename the columns\n",
    "df.rename(rename_dict, inplace=True, axis=1)\n",
    "\n",
    "# reorder the columns (experts, regular, gpt-4)\n",
    "df = df[['Generate a response that...', 'Human Expert 1', 'Human Expert 2', 'Human Regular 1', 'Human Regular 2', 'GPT-4-Turbo', 'Max diff']]\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=df.head(5),\n",
    "    title=\"Personality traits encouraged by different annotators on MultiPref (Strength)\",\n",
    "    first_col_width=0.15,\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"003_cross_annotator_comparison_multipref.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model analysis\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "results_path = pathlib.Path(\"exp/outputs/2025-05-15_13-46-39_mc_v2/results/070_annotations_train_ap.json\")\n",
    "\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(results_path)\n",
    "\n",
    "annotators = dataset.get_available_annotators()\n",
    "model_anns = {\n",
    "    k: v for k, v in annotators.items() if v[\"variant\"] == \"model_identity\"\n",
    "}\n",
    "\n",
    "dataset.set_annotator_cols(annotator_keys=list(model_anns.keys()))\n",
    "metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "# remove the 070_annotations_train_ap from each column name\n",
    "metrics_df.columns = metrics_df.columns.str.replace(\"070_annotations_train_ap\\n(Model: \", \"\").str.replace(\")\", \"\").str.replace(\"openrouter/\", \"\").str.replace(\" \", \"-\")\n",
    "\n",
    "#Â rename max-diff to Max diff\n",
    "metrics_df.rename(columns={\"Max-diff\": \"Max diff\"}, inplace=True)\n",
    "\n",
    "#Â set all gpt-4o models to 0\n",
    "metrics_df[\"openai/gpt-4o-2024-08-06\"] = 0\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=metrics_df.head(5),\n",
    "    title=\"Most diverging personality traits across models\",\n",
    "    first_col_width=0.15,\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"004_model_comparison.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(metrics_df.head(5).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "short_names = {\n",
    "    'meta-llama/llama-3-70b-instruct': \"Llama-3-70b\",\n",
    "    'meta-llama/llama-3.3-70b-instruct':  \"Llama-3.3-70b\",\n",
    "    'meta-llama/llama-4-maverick': \"Llama-4-Maverick\",\n",
    "    'mistralai/mistral-medium': \"Mistral-Medium\",\n",
    "    'mistralai/mistral-medium-3': \"Mistral-Medium-3\",\n",
    "    'openai/gpt-4.1-2025-04-14': \"GPT-4.1\",\n",
    "    'openai/gpt-4o-2024-08-06': \"GPT-4o\",\n",
    "    'meta-llama/llama-2-70b-chat': \"Llama-2-70b\",\n",
    "    'openai/gpt-3.5-turbo': \"GPT-3.5-Turbo\",\n",
    "    'mistralai/mistral-7b-instruct-v0.1': \"Mistral-7b\",\n",
    "}\n",
    "\n",
    "\n",
    "def plot_model_comparison_by_family(metrics_df, trait_index=\"makes more confident statements\", ax=None, top_margin=0.25, bottom_margin=0.25):\n",
    "    # Import adjustText for automatic label positioning\n",
    "    from adjustText import adjust_text\n",
    "\n",
    "    # Create axis if not provided\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(3, 2))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    # Extract data for the plot\n",
    "    data = metrics_df.loc[trait_index].drop(\"Max diff\")\n",
    "    # Group models by family\n",
    "    model_families = {\n",
    "        \"Meta\": sorted([col for col in data.index if \"llama\" in col.lower()]),\n",
    "        \"Mistral\": sorted([col for col in data.index if \"mistral\" in col.lower()]),\n",
    "        \"OpenAI\": sorted([col for col in data.index if \"gpt\" in col.lower()],\n",
    "                  key=lambda x: 0 if \"gpt-4o\" in x.lower() else 1 if \"gpt-4.1\" in x.lower() else -1)\n",
    "    }\n",
    "\n",
    "    # Plot each model family with a different color\n",
    "\n",
    "    Color1 = \"#9eb0ff\"  # Light blue\n",
    "    Color2 = \"#ffadad\"  # Light red\n",
    "    Color3 = \"#84cb75\"  # Light green\n",
    "\n",
    "    colors = [Color1, Color2, Color3]\n",
    "    markers = ['o', 's', '^']\n",
    "\n",
    "    # Store text objects for adjustText and lines for objects to avoid\n",
    "    texts = []\n",
    "    line_objects = []\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "\n",
    "    # Calculate x positions for each family\n",
    "    family_centers = []\n",
    "    family_widths = []\n",
    "\n",
    "    # First pass to determine x positions\n",
    "    start_x = 0\n",
    "    for i, (family, models) in enumerate(model_families.items()):\n",
    "        if len(models) > 1:\n",
    "            width = len(models)\n",
    "        else:\n",
    "            width = 1\n",
    "\n",
    "        family_widths.append(width)\n",
    "        family_centers.append(start_x + width/2)\n",
    "        start_x += width + 1  # Add spacing between families\n",
    "\n",
    "    for i, (family, models) in enumerate(model_families.items()):\n",
    "        family_data = data[models]\n",
    "        # Calculate x positions for this family\n",
    "        if len(models) > 1:\n",
    "            x = np.linspace(family_centers[i] - family_widths[i]/2 + 0.5,\n",
    "                           family_centers[i] + family_widths[i]/2 - 0.5,\n",
    "                           len(models))\n",
    "        else:\n",
    "            x = np.array([family_centers[i]])\n",
    "\n",
    "        line, = ax.plot(x, family_data.values, marker=markers[i], linestyle='-',\n",
    "                 color=colors[i], linewidth=2, markersize=8)\n",
    "        line_objects.append(line)\n",
    "\n",
    "        # Add model names as labels\n",
    "        for j, model in enumerate(models):\n",
    "            label = short_names[model]\n",
    "            text_obj = ax.text(x[j], family_data.values[j],\n",
    "                     label,\n",
    "                     ha='center', va='center', fontsize=8)\n",
    "            texts.append(text_obj)\n",
    "\n",
    "            x_coords.append(x[j])\n",
    "            y_coords.append(family_data.values[j])\n",
    "\n",
    "    # Add vertical padding to the plot\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    y_range = y_max - y_min\n",
    "    top_padding = top_margin * y_range\n",
    "    bottom_padding = bottom_margin * y_range\n",
    "    ax.set_ylim(y_min - bottom_padding, y_max + top_padding)\n",
    "\n",
    "\n",
    "    # Add horizontal padding to the plot\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    x_range = x_max - x_min\n",
    "    padding = x_range * 0.15  # 15% padding\n",
    "    ax.set_xlim(x_min - padding, x_max + padding)\n",
    "\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7, axis='y')\n",
    "    ax.set_ylabel(f'Strength of trait')\n",
    "\n",
    "\n",
    "    # Set x-ticks at the center of each family group with family names\n",
    "    ax.set_xticks(family_centers)\n",
    "    ax.set_xticklabels(model_families.keys())\n",
    "    ax.tick_params(axis='x', which='both', length=0)  # Make tick marks invisible but keep labels\n",
    "\n",
    "    ax.set_yticks([], minor=True)\n",
    "    ax.set_title(f'{trait_index.capitalize()}')\n",
    "\n",
    "    ax.tick_params(right=False)\n",
    "\n",
    "    # No legend needed as we're using x-axis labels\n",
    "\n",
    "    # Use adjustText to automatically position labels without overlap\n",
    "    adjust_text(texts,\n",
    "                objects=line_objects,\n",
    "                arrowprops=dict(arrowstyle='-', color='gray', lw=0.5),\n",
    "                ax=ax,\n",
    "                #expand=(1.2, 1.2),\n",
    "                #avoid_self=True,\n",
    "                force_text=(0.5, 0.4),\n",
    "                #force_explode=(0.5, 0.5)\n",
    "                time_lim=1,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_multiple_traits_side_by_side(metrics_df, traits, ncols=2, top_margin=0.25, bottom_margin=0.25):\n",
    "    \"\"\"\n",
    "    Plot multiple trait comparisons side by side\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics_df : DataFrame\n",
    "        The metrics dataframe\n",
    "    traits : list\n",
    "        List of traits to plot\n",
    "    ncols : int\n",
    "        Number of columns in the grid layout\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fig : Figure\n",
    "        The matplotlib figure\n",
    "    \"\"\"\n",
    "    nrows = (len(traits) + ncols - 1) // ncols  # Calculate number of rows needed\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(2.5 * ncols, 2 * nrows))\n",
    "\n",
    "    # Make axes iterable even if there's only one subplot\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each trait\n",
    "    for i, trait in enumerate(traits):\n",
    "        if i < len(axes):\n",
    "            plot_model_comparison_by_family(metrics_df, trait, ax=axes[i], top_margin=top_margin, bottom_margin=bottom_margin)\n",
    "            # Remove y label on the second plot in each row\n",
    "            if i % ncols != 0:\n",
    "                axes[i].set_ylabel('')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(len(traits), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage - plot for confidence\n",
    "traits = [\n",
    "    \"makes more confident statements\",\n",
    "    \"has a friendlier tone\",\n",
    "    \"provides a numbered list format\",\n",
    "    \"ends with a follow-up question\",\n",
    "    \"expresses more emotion\",\n",
    "]\n",
    "\n",
    "traits = ['has more structured formatting', 'is more verbose',\n",
    "       'makes more confident statements', \"has a friendlier tone\",\n",
    "       'more strictly follows the requested output format',\n",
    "       'is more factually correct', 'is more concise',\n",
    "       'contains less harmful information', 'provides more examples',\n",
    "       'provides a numbered list format',\n",
    "       'is more polite', 'uses more formal language',\n",
    "       'uses more personal pronouns (I, we, you)',\n",
    "       'ends with a follow-up question', 'uses more bold and italics text',\n",
    "       'actively engages the reader with rhetorical questions',\n",
    "       'is more creative and original', 'refuses to answer the question',\n",
    "       \"compliments the user's question or prompt\",\n",
    "       'is more empathetic to the user', 'expresses more emotion',\n",
    "       'agrees more with the user',\n",
    "       'provides conclusions without full reasoning',\n",
    "       'uses a more enthusiastic tone',\n",
    "       'includes more references to other sources', 'is more optimistic',\n",
    "       'more actively engages with the user',\n",
    "       'includes more ethical considerations', 'uses more casual language',\n",
    "       'uses more mathematical symbols and notation', 'uses more humour',\n",
    "       'acknowledges own limitations or uncertainty more', 'uses more emojis',\n",
    "       'has a more avoidant tone', 'includes inappropriate language',\n",
    "       'is more offensive', 'suggests illegal activities',\n",
    "       \"reinforces user's beliefs more\", \"reinforces user's anger more\",\n",
    "       'agrees with user even if factually incorrect']\n",
    "\n",
    "comparison_path = fig_save_path / \"model_comparison\"\n",
    "comparison_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#for trait in traits:\n",
    "#    fig = plot_model_comparison_by_family(metrics_df, trait)\n",
    "#    fig.savefig(comparison_path / (trait.replace(\" \", \"_\") + \".png\"), dpi=300)\n",
    "\n",
    "\n",
    "# Create double figures with two traits side by side\n",
    "for i in range(0, len(traits), 2):\n",
    "    if i + 1 < len(traits):  # Make sure we have a pair\n",
    "        trait_group = [traits[i], traits[i+1]]\n",
    "\n",
    "\n",
    "        fig = plot_multiple_traits_side_by_side(metrics_df, trait_group, ncols=2, top_margin=0.45, bottom_margin=0.15)\n",
    "        fig.savefig(comparison_path / f\"double_figure_{i//2}_topheavy.png\", dpi=300)\n",
    "\n",
    "        fig = plot_multiple_traits_side_by_side(metrics_df, trait_group, ncols=2, top_margin=0.55, bottom_margin=0.15)\n",
    "        fig.savefig(comparison_path / f\"double_figure_{i//2}_megatopheavy.png\", dpi=300)\n",
    "\n",
    "        fig = plot_multiple_traits_side_by_side(metrics_df, trait_group, ncols=2, top_margin=0.15, bottom_margin=0.45)\n",
    "        fig.savefig(comparison_path / f\"double_figure_{i//2}_bottomheavy.png\", dpi=300)\n",
    "\n",
    "        fig = plot_multiple_traits_side_by_side(metrics_df, trait_group, ncols=2, top_margin=0.20, bottom_margin=0.20)\n",
    "        fig.savefig(comparison_path / f\"double_figure_{i//2}_centered.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get standard personality prompts\n",
    "\n",
    "import inverse_cai.experiment.config.default_principles as dp\n",
    "\n",
    "dp.DEFAULT_PRINCIPLES[\"v4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model analysis\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "results_path = pathlib.Path(\"exp/outputs/2025-05-15_13-46-39_mc_v2/results/070_annotations_train_ap.json\")\n",
    "\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(results_path)\n",
    "\n",
    "annotators = dataset.get_available_annotators()\n",
    "model_anns = {\n",
    "    k: v for k, v in annotators.items() if v[\"variant\"] == \"model_identity\"\n",
    "}\n",
    "\n",
    "short_names = {\n",
    "    'meta-llama/llama-3-70b-instruct': \"Llama-3-70b\",\n",
    "    'meta-llama/llama-3.3-70b-instruct':  \"Llama-3.3-70b\",\n",
    "    'meta-llama/llama-4-maverick': \"Llama-4-Maverick\",\n",
    "    'mistralai/mistral-medium': \"Mistral-Medium\",\n",
    "    'mistralai/mistral-medium-3': \"Mistral-Medium-3\",\n",
    "    'openai/gpt-4.1-2025-04-14': \"GPT-4.1\",\n",
    "    'openai/gpt-4o-2024-08-06': \"GPT-4o\",\n",
    "    'meta-llama/llama-2-70b-chat': \"Llama-2-70b\",\n",
    "    'openai/gpt-3.5-turbo': \"GPT-3.5-Turbo\",\n",
    "    'mistralai/mistral-7b-instruct-v0.1': \"Mistral-7b\",\n",
    "    'Max diff': \"Max diff\",\n",
    "    \"Generate-a-response-that...\": \"Generate a response that...\",\n",
    "}\n",
    "\n",
    "\n",
    "for provider in [\"Meta\", \"Mistral\", \"OpenAI\"]:\n",
    "\n",
    "    prov_model_anns = {key: ann for key, ann in model_anns.items() if provider.lower() in ann[\"model_id\"].lower()}\n",
    "\n",
    "    dataset = ff.DatasetHandler(cache=cache)\n",
    "    dataset.add_data_from_path(results_path)\n",
    "\n",
    "    dataset.set_annotator_cols(annotator_keys=list(prov_model_anns.keys()))\n",
    "    metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "    # remove the 070_annotations_train_ap from each column name\n",
    "    metrics_df.columns = metrics_df.columns.str.replace(\"070_annotations_train_ap\\n(Model: \", \"\").str.replace(\")\", \"\").str.replace(\"openrouter/\", \"\").str.replace(\" \", \"-\")\n",
    "    metrics_df.rename(columns={\"Max-diff\": \"Max diff\"}, inplace=True)\n",
    "\n",
    "    print(metrics_df.columns)\n",
    "\n",
    "    metrics_df.columns = metrics_df.columns.map(short_names)\n",
    "\n",
    "    # sort column alphabetically\n",
    "    metrics_df = metrics_df.reindex(sorted(metrics_df.columns), axis=1)\n",
    "    # but keep the Generate a response that... column first\n",
    "    metrics_df = metrics_df[[\"Generate a response that...\", *[col for col in metrics_df.columns if col != \"Generate a response that...\"]]]\n",
    "    # ensure the max diff column is last\n",
    "    metrics_df = metrics_df[[\"Generate a response that...\", *[col for col in metrics_df.columns if col != \"Generate a response that...\" and col != \"Max diff\"], \"Max diff\"]]\n",
    "\n",
    "    print(metrics_df.columns)\n",
    "\n",
    "    if \"GPT-4o\" in metrics_df.columns:\n",
    "        # since this is the reference model, set it to 0 - otherwise different frame of reference\n",
    "        metrics_df[\"GPT-4o\"] = 0.0\n",
    "\n",
    "    latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "        metrics_df=metrics_df.head(40),\n",
    "        title=f\"Personality traits across {provider} models\",\n",
    "        first_col_width=0.3,\n",
    "    )\n",
    "\n",
    "    with open(tex_app_save_path / f\"010_model_comparison_{provider}.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
