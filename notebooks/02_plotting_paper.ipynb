{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedback_forensics as ff\n",
    "import feedback_forensics.app.plotting.paper as paper_plot\n",
    "import pathlib\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "data_path = pathlib.Path(\"../forensics-data/feedback-forensics-public-results\")\n",
    "fig_save_path = pathlib.Path(\"./output/figures\")\n",
    "tex_save_path = pathlib.Path(\"./output/tex\")\n",
    "\n",
    "# ensure save path exists\n",
    "fig_save_path.mkdir(parents=True, exist_ok=True)\n",
    "tex_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# save general latex preamble\n",
    "with open(tex_save_path / \"000_preamble.tex\", \"w\") as f:\n",
    "    f.write(paper_plot.get_latex_doc_preamble())\n",
    "\n",
    "# example latex table\n",
    "with open(tex_save_path / \"999_example_table.tex\", \"w\") as f:\n",
    "    latex = []\n",
    "    latex = paper_plot.add_table_preamble(latex, \"Example Table\")\n",
    "    latex = paper_plot.add_table_postamble(latex)\n",
    "    f.write(\"\\n\".join(latex))\n",
    "\n",
    "datasets = [\n",
    "    [\"multipref_10k_v3.json\", \"MultiPref\"],\n",
    "    [\"llama4_arena_vs_public_version.json\", \"Llama 4 Arena vs Public\"],\n",
    "    [\"arena\", \"Chatbot Arena\"],\n",
    "    [\"prism\", \"PRISM\"],\n",
    "]\n",
    "\n",
    "cache = {}\n",
    "for dataset_path, dataset_name in datasets:\n",
    "    dataset = ff.DatasetHandler(cache=cache)\n",
    "    dataset.add_data_from_path(data_path / dataset_path)\n",
    "\n",
    "    overall_metrics = dataset.get_overall_metrics()\n",
    "    annotator_metrics = dataset.get_annotator_metrics()\n",
    "\n",
    "    metric_name = \"strength\"\n",
    "    strength_metrics = annotator_metrics[dataset_path.split(\".\")[0]][\"metrics\"][metric_name]\n",
    "\n",
    "    kwargs = {}\n",
    "    if dataset_path == \"llama4_arena_vs_public_version.json\":\n",
    "        kwargs = {\n",
    "            \"top_title\": \"Traits stronger in arena relative to public model\",\n",
    "            \"bottom_title\": \"Traits weaker in arena relative to public model\",\n",
    "        }\n",
    "\n",
    "    latex_table = paper_plot.get_latex_top_and_bottom_annotators(\n",
    "        annotator_metrics=strength_metrics,\n",
    "        metric_name=metric_name.capitalize(),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with open(tex_save_path / f\"001_top_and_bottom_annotators_{dataset_path.split('.')[0]}.tex\", \"w\") as f:\n",
    "        f.write(latex_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Arena data\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"arena\"\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(data_path / dataset_name)\n",
    "\n",
    "general_df = dataset.first_handler.df\n",
    "values = [\n",
    "    'Creative Writing Prompts',\n",
    "    'Songwriting Prompts',\n",
    "    'Resume and Cover Letter Writing',\n",
    "    'Professional Email Communication',\n",
    "]\n",
    "dataset.split_by_col(col=\"narrower_category\", selected_vals=values)\n",
    "\n",
    "metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=metrics_df.head(10),\n",
    "    title=\"Encouraged personality traits across writing domains in Chatbot Arena (Strength)\",\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"002_writing_tasks_arena.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of MultiPref data\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "cache = {}\n",
    "data_path = pathlib.Path(\"../forensics-data/feedback-forensics-public-results\")\n",
    "\n",
    "dataset_name = \"multipref_10k_v3.json\"\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_metadata = dataset.get_available_annotators()\n",
    "special_annotators = {\n",
    "    annotator_key: metadata\n",
    "    for annotator_key, metadata in annotator_metadata.items()\n",
    "    if metadata[\"variant\"] in [\"unknown\", \"human\"]\n",
    "    if \"normal\" in metadata[\"annotator_visible_name\"] or \"expert\" in metadata[\"annotator_visible_name\"] or \"gpt4\" in metadata[\"annotator_visible_name\"]\n",
    "}\n",
    "special_annotators\n",
    "\n",
    "dataset.set_annotator_cols(annotator_keys=list(special_annotators.keys()))\n",
    "df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'multipref_10k_v3\\n(unknown: expert_1_preferred_text)': 'Human Expert 2',\n",
    "    'multipref_10k_v3\\n(unknown: preferred_text_gpt4)': 'GPT-4',\n",
    "    'multipref_10k_v3\\n(unknown: normal_0_preferred_text)': 'Human Regular 1',\n",
    "    'multipref_10k_v3\\n(unknown: normal_1_preferred_text)': 'Human Regular 2',\n",
    "    'multipref_10k_v3\\n(unknown: expert_0_preferred_text)': 'Human Expert 1',\n",
    "}\n",
    "# rename the columns\n",
    "df.rename(rename_dict, inplace=True, axis=1)\n",
    "\n",
    "# reorder the columns (experts, regular, gpt-4)\n",
    "df = df[['Generate a response that...', 'Human Expert 1', 'Human Expert 2', 'Human Regular 1', 'Human Regular 2', 'GPT-4', 'Max diff']]\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=df.head(5),\n",
    "    title=\"Personality traits encouraged by different annotators on MultiPref (Strength)\",\n",
    "    first_col_width=0.15,\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"003_cross_annotator_comparison_multipref.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model analysis\n",
    "import pandas as pd\n",
    "import feedback_forensics as ff\n",
    "import pathlib\n",
    "\n",
    "results_path = pathlib.Path(\"exp/outputs/2025-05-12_14-46-21_mc_v1/results/070_annotations_train_ap.json\")\n",
    "\n",
    "dataset = ff.DatasetHandler(cache=cache)\n",
    "dataset.add_data_from_path(results_path)\n",
    "\n",
    "annotators = dataset.get_available_annotators()\n",
    "model_anns = {\n",
    "    k: v for k, v in annotators.items() if v[\"variant\"] == \"model_identity\"\n",
    "}\n",
    "\n",
    "dataset.set_annotator_cols(annotator_keys=list(model_anns.keys()))\n",
    "metrics_df = dataset.get_annotator_metrics_df(metric_name=\"strength\", index_col_name=\"Generate a response that...\")\n",
    "\n",
    "# remove the 070_annotations_train_ap from each column name\n",
    "metrics_df.columns = metrics_df.columns.str.replace(\"070_annotations_train_ap\\n(Model: \", \"\").str.replace(\")\", \"\").str.replace(\"openrouter/\", \"\").str.replace(\" \", \"-\")\n",
    "\n",
    "# rename max-diff to Max diff\n",
    "metrics_df.rename(columns={\"Max-diff\": \"Max diff\"}, inplace=True)\n",
    "\n",
    "# set all gpt-4o models to 0\n",
    "metrics_df[\"openai/gpt-4o-2024-08-06\"] = 0\n",
    "\n",
    "latex_str = paper_plot.get_latex_table_from_metrics_df(\n",
    "    metrics_df=metrics_df.head(5),\n",
    "    title=\"Most diverging personality traits across models\",\n",
    "    first_col_width=0.15,\n",
    ")\n",
    "\n",
    "with open(tex_save_path / \"004_model_comparison.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(metrics_df.head(5).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "short_names = {\n",
    "    'meta-llama/llama-3-70b-instruct': \"Llama-3-70b\",\n",
    "    'meta-llama/llama-3.3-70b-instruct':  \"Llama-3.3-70b\",\n",
    "    'meta-llama/llama-4-maverick': \"Llama-4-Maverick\",\n",
    "    'mistralai/mistral-medium': \"Mistral-Medium\",\n",
    "    'mistralai/mistral-medium-3': \"Mistral-Medium-3\",\n",
    "    'openai/gpt-4.1-2025-04-14': \"GPT-4.1\",\n",
    "    'openai/gpt-4o-2024-08-06': \"GPT-4o\",\n",
    "    'meta-llama/llama-2-70b-chat': \"Llama-2-70b\",\n",
    "    'openai/gpt-3.5-turbo': \"GPT-3.5-Turbo\",\n",
    "    'mistralai/mistral-7b-instruct-v0.1': \"Mistral-7b\",\n",
    "}\n",
    "\n",
    "\n",
    "def plot_model_comparison_by_family(metrics_df, trait_index=\"makes more confident statements\"):\n",
    "    # Extract data for the plot\n",
    "    data = metrics_df.loc[trait_index].drop(\"Max diff\")\n",
    "    # Group models by family\n",
    "    model_families = {\n",
    "        \"Meta\": sorted([col for col in data.index if \"llama\" in col.lower()]),\n",
    "        \"Mistral\": sorted([col for col in data.index if \"mistral\" in col.lower()]),\n",
    "        \"OpenAI\": sorted([col for col in data.index if \"gpt\" in col.lower()],\n",
    "                  key=lambda x: 0 if \"gpt-4o\" in x.lower() else 1 if \"gpt-4.1\" in x.lower() else -1)\n",
    "    }\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "\n",
    "    # Plot each model family with a different color\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green\n",
    "    markers = ['o', 's', '^']\n",
    "\n",
    "    for i, (family, models) in enumerate(model_families.items()):\n",
    "        family_data = data[models]\n",
    "        # Normalize x positions to ensure all model families start at 0 and end at 1\n",
    "        if len(models) > 1:\n",
    "            x = np.linspace(0, 1, len(models)) + 1.2 * i\n",
    "        else:\n",
    "            x = np.array([0.5])  # Center a single model\n",
    "        plt.plot(x, family_data.values, marker=markers[i], linestyle='-',\n",
    "                 label=family, color=colors[i], linewidth=2, markersize=8)\n",
    "\n",
    "        # Add model names as x-tick labels\n",
    "        for j, model in enumerate(models):\n",
    "            label = short_names[model]\n",
    "            plt.text(x[j], family_data.values[j],\n",
    "                     label,\n",
    "                     rotation=-25, ha='right', va='bottom', fontsize=8)\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.ylabel(f'Strength of trait (rel. to ref. model)')\n",
    "    plt.xticks([])\n",
    "    plt.yticks(plt.yticks()[0], minor=True)\n",
    "    plt.tick_params(axis='y', which='minor', left=False)\n",
    "    plt.title(f'Model {trait_index} - Comparison by Family')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Example usage - plot for confidence\n",
    "traits = [\n",
    "    \"makes more confident statements\",\n",
    "    \"provides a numbered list format\",\n",
    "    \"has a friendlier tone\",\n",
    "    \"ends with a follow-up question\",\n",
    "    \"expresses more emotion\",\n",
    "]\n",
    "\n",
    "for trait in traits:\n",
    "    plot_model_comparison_by_family(metrics_df, trait).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
