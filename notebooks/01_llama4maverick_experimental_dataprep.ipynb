{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:['question_id', 'model_a', 'model_b', 'winner', 'judge', 'conversation_a', 'conversation_b', 'turn', 'anony', 'language', 'tstamp', 'conv_metadata', 'is_code', 'is_refusal', 'metadata_a', 'metadata_b', 'dedup_tag', 'category_tag', 'category', 'outcome', 'opponent', 'redacted_messages_a', 'redacted_messages_b', 'redacted']\n",
      "Length:2382\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>judge</th>\n",
       "      <th>conversation_a</th>\n",
       "      <th>conversation_b</th>\n",
       "      <th>turn</th>\n",
       "      <th>anony</th>\n",
       "      <th>language</th>\n",
       "      <th>...</th>\n",
       "      <th>metadata_a</th>\n",
       "      <th>metadata_b</th>\n",
       "      <th>dedup_tag</th>\n",
       "      <th>category_tag</th>\n",
       "      <th>category</th>\n",
       "      <th>outcome</th>\n",
       "      <th>opponent</th>\n",
       "      <th>redacted_messages_a</th>\n",
       "      <th>redacted_messages_b</th>\n",
       "      <th>redacted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>model_b</td>\n",
       "      <td>46c8580211c21011fbb509d9d2bc9945</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>French</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'criteria_v0.1': {'specificity': False, 'doma...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental battles</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental Won</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8745be6affc47bfb94120b193600a66</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>model_b</td>\n",
       "      <td>8d99ff45127a5424099074ab1107dff2</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'criteria_v0.1': {'specificity': True, 'domai...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental battles</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental Won</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65aaba9761bf4dd2be96cb77c6f5f8a6</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>11ab977c0f5b55a32f7a7b26a5fa54f1</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'criteria_v0.1': {'specificity': True, 'domai...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental battles</td>\n",
       "      <td>Tie</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                              model_a  \\\n",
       "0  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "1  e8745be6affc47bfb94120b193600a66               llama-3.3-70b-instruct   \n",
       "2  65aaba9761bf4dd2be96cb77c6f5f8a6  Llama-4-Maverick-03-26-Experimental   \n",
       "\n",
       "                               model_b         winner  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental        model_b   \n",
       "1  Llama-4-Maverick-03-26-Experimental        model_b   \n",
       "2                                phi-4  tie (bothbad)   \n",
       "\n",
       "                              judge  \\\n",
       "0  46c8580211c21011fbb509d9d2bc9945   \n",
       "1  8d99ff45127a5424099074ab1107dff2   \n",
       "2  11ab977c0f5b55a32f7a7b26a5fa54f1   \n",
       "\n",
       "                                      conversation_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...   \n",
       "\n",
       "                                      conversation_b  turn  anony language  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...     1   True   French   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...     1   True  Chinese   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...     1   True  English   \n",
       "\n",
       "   ...  metadata_a metadata_b                              dedup_tag  \\\n",
       "0  ...        None       None  {'high_freq': False, 'sampled': True}   \n",
       "1  ...        None       None  {'high_freq': False, 'sampled': True}   \n",
       "2  ...        None       None  {'high_freq': False, 'sampled': True}   \n",
       "\n",
       "                                        category_tag  \\\n",
       "0  {'criteria_v0.1': {'specificity': False, 'doma...   \n",
       "1  {'criteria_v0.1': {'specificity': True, 'domai...   \n",
       "2  {'criteria_v0.1': {'specificity': True, 'domai...   \n",
       "\n",
       "                                      category  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental battles   \n",
       "1  Llama-4-Maverick-03-26-Experimental battles   \n",
       "2  Llama-4-Maverick-03-26-Experimental battles   \n",
       "\n",
       "                                   outcome                opponent  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental Won  llama-3.3-70b-instruct   \n",
       "1  Llama-4-Maverick-03-26-Experimental Won  llama-3.3-70b-instruct   \n",
       "2                                      Tie                   phi-4   \n",
       "\n",
       "                                 redacted_messages_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...   \n",
       "\n",
       "                                 redacted_messages_b redacted  \n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...    False  \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...    False  \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...    False  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Arena data\n",
    "# Llama-4-Maverick-03-26-Experimental vs other models\n",
    "# From https://huggingface.co/spaces/lmarena-ai/Llama-4-Maverick-03-26-Experimental_battles/tree/main/data\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "arena_path = pathlib.Path(\"../data/input/clean-llama4.jsonl\")\n",
    "data = []\n",
    "with open(arena_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "arena_df = pd.DataFrame(data)\n",
    "print(f\"Columns:{list(arena_df.columns)}\")\n",
    "print(f\"Length:{len(arena_df)}\")\n",
    "arena_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample conversation_a keys: dict_keys(['role', 'content'])\n",
      "Sample conversation_b keys: dict_keys(['role', 'content'])\n",
      "num_tokens still present in conversation_a: False\n",
      "num_tokens still present in conversation_b: False\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning: remove the token number from the conversations\n",
    "def remove_num_tokens(x):\n",
    "    \"\"\"\n",
    "    Remove 'num_tokens' field from conversation data which can be in different formats:\n",
    "    - string representation of a list/dict\n",
    "    - list of dictionaries\n",
    "    - dictionary\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        # Handle string representation of data\n",
    "        parsed_data = json.loads(json.dumps(eval(x)))\n",
    "        return [{k: v for k, v in item.items() if k != 'num_tokens'} for item in parsed_data]\n",
    "    elif isinstance(x, list):\n",
    "        # Handle list of dictionaries\n",
    "        return [{k: v for k, v in item.items() if k != 'num_tokens'} for item in x]\n",
    "    else:\n",
    "        # Return unchanged if none of the above\n",
    "        print(f\"Unknown type: {type(x)}\")\n",
    "        print(f\"beginning of x: {x[:100]}\")\n",
    "        print(f\"end of x: {x[-100:]}\")\n",
    "        return x\n",
    "\n",
    "# Apply the function to both conversation columns\n",
    "for col in [\"conversation_a\", \"conversation_b\"]:\n",
    "    arena_df[col] = arena_df[col].apply(remove_num_tokens)\n",
    "\n",
    "\n",
    "# spot check if num_tokens is present in the conversation_a and conversation_b columns\n",
    "sample_a = arena_df[\"conversation_a\"].iloc[0]\n",
    "sample_b = arena_df[\"conversation_b\"].iloc[0]\n",
    "\n",
    "if isinstance(sample_a, list) and len(sample_a) > 0:\n",
    "    print(f\"Sample conversation_a keys: {sample_a[0].keys()}\")\n",
    "if isinstance(sample_b, list) and len(sample_b) > 0:\n",
    "    print(f\"Sample conversation_b keys: {sample_b[0].keys()}\")\n",
    "\n",
    "# Check if num_tokens was successfully removed\n",
    "has_num_tokens_a = isinstance(sample_a, list) and len(sample_a) > 0 and 'num_tokens' in sample_a[0]\n",
    "has_num_tokens_b = isinstance(sample_b, list) and len(sample_b) > 0 and 'num_tokens' in sample_b[0]\n",
    "print(f\"num_tokens still present in conversation_a: {has_num_tokens_a}\")\n",
    "print(f\"num_tokens still present in conversation_b: {has_num_tokens_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of only single turn conversation: 2069\n"
     ]
    }
   ],
   "source": [
    "# get just the prompt (we only generate for single-turn for simplicity)\n",
    "arena_singleturn_df = arena_df[arena_df[\"turn\"] == 1].copy()\n",
    "print(f\"Len of only single turn conversation: {len(arena_singleturn_df)}\")\n",
    "arena_singleturn_df.loc[:, \"prompt_a\"] = arena_singleturn_df[\"conversation_a\"].apply(\n",
    "    lambda x: x[0]['content'] if isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict) and 'content' in x[0] else None\n",
    ")\n",
    "arena_singleturn_df.loc[:, \"prompt_b\"] = arena_singleturn_df[\"conversation_b\"].apply(\n",
    "    lambda x: x[0]['content'] if isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict) and 'content' in x[0] else None\n",
    ")\n",
    "\n",
    "# sanity check: prompt_a should equal prompt_b\n",
    "assert arena_singleturn_df[\"prompt_a\"].equals(arena_singleturn_df[\"prompt_b\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2069/2069 [00:00<00:00, 27072.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate new responses using openrouter Llama 4 Maverick\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import inverse_cai.models\n",
    "\n",
    "model = inverse_cai.models.get_model(\"openrouter/meta-llama/llama-4-maverick\", max_tokens=100000)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"data/output\", exist_ok=True)\n",
    "\n",
    "# Open a file to write the results\n",
    "output_file = \"../data/output/llama4_maverick_openrouter_responses.jsonl\"\n",
    "\n",
    "# Process each prompt and save the response\n",
    "# Check if file exists to continue from where we left off\n",
    "import os\n",
    "\n",
    "# Function to get already processed prompt IDs\n",
    "def get_processed_ids():\n",
    "    processed_ids = set()\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    processed_ids.add(record[\"question_id\"])\n",
    "                except:\n",
    "                    pass\n",
    "    return processed_ids\n",
    "\n",
    "# Get already processed prompt IDs\n",
    "processed_ids = get_processed_ids()\n",
    "\n",
    "# Open file in append mode to continue where we left off\n",
    "with open(output_file, \"a\") as f:\n",
    "    for idx, row in tqdm(arena_singleturn_df.iterrows(), total=len(arena_singleturn_df)):\n",
    "        question_id = row[\"question_id\"]\n",
    "        prompt = row[\"prompt_a\"]\n",
    "\n",
    "        # Skip if already processed\n",
    "        if question_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Create message and generate response\n",
    "            msg = {\"role\": \"user\", \"content\": prompt}\n",
    "            generation = model.invoke([msg])\n",
    "\n",
    "            # Create a record with prompt, response, and metadata\n",
    "            record = {\n",
    "                \"question_id\": question_id,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generation.content,\n",
    "                \"full_response\": dict(generation),\n",
    "                \"model\": \"meta-llama/llama-4-maverick\"\n",
    "            }\n",
    "\n",
    "            # Write the record to the file\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "            # Add to processed set\n",
    "            processed_ids.add(question_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {question_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>full_response</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>Sténose de la carotide interne extra crânienne...</td>\n",
       "      <td>La sténose de la carotide interne extra-crânie...</td>\n",
       "      <td>{'content': 'La sténose de la carotide interne...</td>\n",
       "      <td>meta-llama/llama-4-maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8745be6affc47bfb94120b193600a66</td>\n",
       "      <td>续写接下来三章，共8500字，第一章写主角与红豆的，懵懂的恋爱，要读者心痒痒的感觉。团藏出手...</td>\n",
       "      <td>## 第一章 懵懂的恋爱与初次任务\\n\\n春日的暖阳洒在木叶村的街道上，给宁静的村庄带来了一...</td>\n",
       "      <td>{'content': '## 第一章 懵懂的恋爱与初次任务\n",
       "\n",
       "春日的暖阳洒在木叶村的街道上...</td>\n",
       "      <td>meta-llama/llama-4-maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65aaba9761bf4dd2be96cb77c6f5f8a6</td>\n",
       "      <td>Why do most scores written for brass neglect t...</td>\n",
       "      <td>You're right; many scores written for brass in...</td>\n",
       "      <td>{'content': 'You're right; many scores written...</td>\n",
       "      <td>meta-llama/llama-4-maverick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  caedec12e65841c5857d9879f892abb9   \n",
       "1  e8745be6affc47bfb94120b193600a66   \n",
       "2  65aaba9761bf4dd2be96cb77c6f5f8a6   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Sténose de la carotide interne extra crânienne...   \n",
       "1  续写接下来三章，共8500字，第一章写主角与红豆的，懵懂的恋爱，要读者心痒痒的感觉。团藏出手...   \n",
       "2  Why do most scores written for brass neglect t...   \n",
       "\n",
       "                                            response  \\\n",
       "0  La sténose de la carotide interne extra-crânie...   \n",
       "1  ## 第一章 懵懂的恋爱与初次任务\\n\\n春日的暖阳洒在木叶村的街道上，给宁静的村庄带来了一...   \n",
       "2  You're right; many scores written for brass in...   \n",
       "\n",
       "                                       full_response  \\\n",
       "0  {'content': 'La sténose de la carotide interne...   \n",
       "1  {'content': '## 第一章 懵懂的恋爱与初次任务\n",
       "\n",
       "春日的暖阳洒在木叶村的街道上...   \n",
       "2  {'content': 'You're right; many scores written...   \n",
       "\n",
       "                         model  \n",
       "0  meta-llama/llama-4-maverick  \n",
       "1  meta-llama/llama-4-maverick  \n",
       "2  meta-llama/llama-4-maverick  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly generated responses (Llama 4 Maverick via openrouter)\n",
    "data = []\n",
    "with open(output_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "gen_df = pd.DataFrame(data)\n",
    "gen_df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason\n",
      "stop    2069\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# fix formatting to standard form\n",
    "gen_df[\"conversation\"] = gen_df.apply(\n",
    "    lambda row: [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': row['prompt'],\n",
    "        },\n",
    "        {\n",
    "            'role': 'assistant',\n",
    "            'content': row['response'],\n",
    "        }\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Select subset of generations that were not stopped by external factors\n",
    "# (e.g. max tokens)\n",
    "gen_df[\"finish_reason\"] = gen_df[\"full_response\"].apply(\n",
    "        lambda x: x.get(\"response_metadata\", {}).get(\"finish_reason\", None)\n",
    "        if isinstance(x, dict) else None\n",
    "    )\n",
    "\n",
    "print(gen_df[\"finish_reason\"].value_counts())\n",
    "gen_df = gen_df[gen_df[\"finish_reason\"] != \"length\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8745be6affc47bfb94120b193600a66</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65aaba9761bf4dd2be96cb77c6f5f8a6</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>phi-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1d620078d63e4becb3bef25ef545ace3</td>\n",
       "      <td>[{'role': 'user', 'content': 'create me fun we...</td>\n",
       "      <td>[{'role': 'user', 'content': 'create me fun we...</td>\n",
       "      <td>command-a-03-2025</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b31f569c0eec4f75ad52ff6ff1996610</td>\n",
       "      <td>[{'role': 'user', 'content': 'Comment puis-je ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Comment puis-je ...</td>\n",
       "      <td>mistral-small-24b-instruct-2501</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  caedec12e65841c5857d9879f892abb9   \n",
       "1  e8745be6affc47bfb94120b193600a66   \n",
       "2  65aaba9761bf4dd2be96cb77c6f5f8a6   \n",
       "3  1d620078d63e4becb3bef25ef545ace3   \n",
       "4  b31f569c0eec4f75ad52ff6ff1996610   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...   \n",
       "3  [{'role': 'user', 'content': 'create me fun we...   \n",
       "4  [{'role': 'user', 'content': 'Comment puis-je ...   \n",
       "\n",
       "                                          response_b  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...   \n",
       "3  [{'role': 'user', 'content': 'create me fun we...   \n",
       "4  [{'role': 'user', 'content': 'Comment puis-je ...   \n",
       "\n",
       "                               model_a                              model_b  \n",
       "0               llama-3.3-70b-instruct  Llama-4-Maverick-03-26-Experimental  \n",
       "1               llama-3.3-70b-instruct  Llama-4-Maverick-03-26-Experimental  \n",
       "2  Llama-4-Maverick-03-26-Experimental                                phi-4  \n",
       "3                    command-a-03-2025  Llama-4-Maverick-03-26-Experimental  \n",
       "4      mistral-small-24b-instruct-2501  Llama-4-Maverick-03-26-Experimental  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge arena df and new generations into one df\n",
    "# with three model responses per row (if generation available)\n",
    "\n",
    "merged_df = arena_df[[\"question_id\"]].copy()\n",
    "merged_df[\"response_a\"] = arena_df[\"conversation_a\"]\n",
    "merged_df[\"response_b\"] = arena_df[\"conversation_b\"]\n",
    "merged_df[\"model_a\"] = arena_df[\"model_a\"]\n",
    "merged_df[\"model_b\"] = arena_df[\"model_b\"]\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (2382, 7)\n",
      "Number of rows with response_c: 2069\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>response_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>response_b</th>\n",
       "      <th>model_c</th>\n",
       "      <th>response_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8745be6affc47bfb94120b193600a66</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65aaba9761bf4dd2be96cb77c6f5f8a6</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Why do most scor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                              model_a  \\\n",
       "0  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "1  e8745be6affc47bfb94120b193600a66               llama-3.3-70b-instruct   \n",
       "2  65aaba9761bf4dd2be96cb77c6f5f8a6  Llama-4-Maverick-03-26-Experimental   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...   \n",
       "\n",
       "                               model_b  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental   \n",
       "1  Llama-4-Maverick-03-26-Experimental   \n",
       "2                                phi-4   \n",
       "\n",
       "                                          response_b  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...   \n",
       "\n",
       "                       model_c  \\\n",
       "0  llama-4-maverick-openrouter   \n",
       "1  llama-4-maverick-openrouter   \n",
       "2  llama-4-maverick-openrouter   \n",
       "\n",
       "                                          response_c  \n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...  \n",
       "1  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...  \n",
       "2  [{'role': 'user', 'content': 'Why do most scor...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Merge merged_df with gen_df on question_id\n",
    "# Left join to keep all rows from merged_df, even if there's no match in gen_df\n",
    "merged_df = merged_df.merge(\n",
    "    gen_df[[\"question_id\", \"conversation\"]],\n",
    "    left_on=\"question_id\",\n",
    "    right_on=\"question_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename the conversation column to response_c for consistency\n",
    "merged_df.rename(columns={\"conversation\": \"response_c\"}, inplace=True)\n",
    "\n",
    "# Add model_c column to indicate the source of response_c\n",
    "merged_df[\"model_c\"] = np.where(merged_df[\"response_c\"].notna(), \"llama-4-maverick-openrouter\", None)\n",
    "\n",
    "print(f\"Merged dataframe shape: {merged_df.shape}\")\n",
    "print(f\"Number of rows with response_c: {merged_df['response_c'].notna().sum()}\")\n",
    "\n",
    "# Reorder columns for better organization\n",
    "merged_df = merged_df[['question_id',\n",
    "                       'model_a', 'response_a',\n",
    "                       'model_b', 'response_b',\n",
    "                       'model_c', 'response_c']]\n",
    "\n",
    "\n",
    "merged_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairwise comparisons: 6520\n",
      "Number of unique questions: 2382\n",
      "Pairings distribution:\n",
      "pairing\n",
      "a_vs_b    2382\n",
      "a_vs_c    2069\n",
      "b_vs_c    2069\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>text_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>text_b</th>\n",
       "      <th>pairing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>a_vs_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>a_vs_c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>b_vs_c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e8745be6affc47bfb94120b193600a66</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>a_vs_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e8745be6affc47bfb94120b193600a66</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': '续写接下来三章，共8500字，第...</td>\n",
       "      <td>a_vs_c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                              model_a  \\\n",
       "0  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "1  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "2  caedec12e65841c5857d9879f892abb9  Llama-4-Maverick-03-26-Experimental   \n",
       "3  e8745be6affc47bfb94120b193600a66               llama-3.3-70b-instruct   \n",
       "4  e8745be6affc47bfb94120b193600a66               llama-3.3-70b-instruct   \n",
       "\n",
       "                                              text_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "2  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "3  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "4  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...   \n",
       "\n",
       "                               model_b  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental   \n",
       "1          llama-4-maverick-openrouter   \n",
       "2          llama-4-maverick-openrouter   \n",
       "3  Llama-4-Maverick-03-26-Experimental   \n",
       "4          llama-4-maverick-openrouter   \n",
       "\n",
       "                                              text_b pairing  \n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...  a_vs_b  \n",
       "1  [{'role': 'user', 'content': 'Sténose de la ca...  a_vs_c  \n",
       "2  [{'role': 'user', 'content': 'Sténose de la ca...  b_vs_c  \n",
       "3  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...  a_vs_b  \n",
       "4  [{'role': 'user', 'content': '续写接下来三章，共8500字，第...  a_vs_c  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a new df with pairings a vs b, a vs c, and b vs c (only skipping specific pairings if a response is missing)\n",
    "pairwise_comparisons = []\n",
    "\n",
    "# Process each row in the merged dataframe\n",
    "for _, row in merged_df.iterrows():\n",
    "\n",
    "    # Check if response exists and is valid\n",
    "    response_a_avail = isinstance(row['response_a'], list) or not pd.isna(row['response_a'])\n",
    "    response_b_avail = isinstance(row['response_b'], list) or not pd.isna(row['response_b'])\n",
    "    response_c_avail = isinstance(row['response_c'], list) or not pd.isna(row['response_c'])\n",
    "\n",
    "    # Create a vs b pairing if both responses exist\n",
    "    if response_a_avail and response_b_avail:\n",
    "        pairwise_comparisons.append({\n",
    "            'question_id': row['question_id'],\n",
    "            'model_a': row['model_a'],\n",
    "            'text_a': row['response_a'],\n",
    "            'model_b': row['model_b'],\n",
    "            'text_b': row['response_b'],\n",
    "            'pairing': 'a_vs_b'\n",
    "        })\n",
    "\n",
    "    # Create a vs c pairing if both responses exist\n",
    "    if response_a_avail and response_c_avail:\n",
    "        pairwise_comparisons.append({\n",
    "            'question_id': row['question_id'],\n",
    "            'model_a': row['model_a'],\n",
    "            'text_a': row['response_a'],\n",
    "            'model_b': row['model_c'],\n",
    "            'text_b': row['response_c'],\n",
    "            'pairing': 'a_vs_c'\n",
    "        })\n",
    "\n",
    "    # Create b vs c pairing if both responses exist\n",
    "    if response_b_avail and response_c_avail:\n",
    "        pairwise_comparisons.append({\n",
    "            'question_id': row['question_id'],\n",
    "            'model_a': row['model_b'],\n",
    "            'text_a': row['response_b'],\n",
    "            'model_b': row['model_c'],\n",
    "            'text_b': row['response_c'],\n",
    "            'pairing': 'b_vs_c'\n",
    "        })\n",
    "\n",
    "# Create the pairwise comparisons dataframe\n",
    "pairwise_df = pd.DataFrame(pairwise_comparisons)\n",
    "\n",
    "# Display information about the pairwise comparisons\n",
    "print(f\"Total number of pairwise comparisons: {len(pairwise_df)}\")\n",
    "print(f\"Number of unique questions: {pairwise_df['question_id'].nunique()}\")\n",
    "print(f\"Pairings distribution:\\n{pairwise_df['pairing'].value_counts()}\")\n",
    "\n",
    "# Display the first few rows\n",
    "pairwise_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique models: 31\n",
      "List of unique models:\n",
      "- Llama-4-Maverick-03-26-Experimental\n",
      "- amazon-nova-lite-v1.0\n",
      "- amazon-nova-micro-v1.0\n",
      "- amazon-nova-pro-v1.0\n",
      "- chatgpt-4o-latest-20250129\n",
      "- chatgpt-4o-latest-20250326\n",
      "- claude-3-5-haiku-20241022\n",
      "- claude-3-5-sonnet-20241022\n",
      "- claude-3-7-sonnet-20250219\n",
      "- command-a-03-2025\n",
      "- deepseek-r1\n",
      "- deepseek-v3-0324\n",
      "- gemini-2.0-flash-001\n",
      "- gemini-2.0-flash-lite-preview-02-05\n",
      "- gemini-2.0-flash-thinking-exp-01-21\n",
      "- gemini-2.5-pro-exp-03-25\n",
      "- gemma-3-27b-it\n",
      "- gpt-4.5-preview-2025-02-27\n",
      "- gpt-4o-mini-2024-07-18\n",
      "- grok-3-preview-02-24\n",
      "- llama-3.1-405b-instruct-bf16\n",
      "- llama-3.3-70b-instruct\n",
      "- llama-4-maverick-openrouter\n",
      "- mistral-large-2411\n",
      "- mistral-small-24b-instruct-2501\n",
      "- o1-2024-12-17\n",
      "- o3-mini\n",
      "- o3-mini-high\n",
      "- phi-4\n",
      "- qwen2.5-max\n",
      "- qwq-32b\n",
      "Added 31 model-specific columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>text_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>text_b</th>\n",
       "      <th>pairing</th>\n",
       "      <th>llama-3.3-70b-instruct</th>\n",
       "      <th>llama-4-maverick-03-26-experimental</th>\n",
       "      <th>llama-4-maverick-openrouter</th>\n",
       "      <th>phi-4</th>\n",
       "      <th>...</th>\n",
       "      <th>amazon-nova-lite-v1.0</th>\n",
       "      <th>o1-2024-12-17</th>\n",
       "      <th>gpt-4.5-preview-2025-02-27</th>\n",
       "      <th>grok-3-preview-02-24</th>\n",
       "      <th>mistral-large-2411</th>\n",
       "      <th>gemini-2.0-flash-lite-preview-02-05</th>\n",
       "      <th>gemini-2.5-pro-exp-03-25</th>\n",
       "      <th>gemini-2.0-flash-thinking-exp-01-21</th>\n",
       "      <th>amazon-nova-micro-v1.0</th>\n",
       "      <th>deepseek-v3-0324</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>a_vs_b</td>\n",
       "      <td>text_a</td>\n",
       "      <td>text_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>a_vs_c</td>\n",
       "      <td>text_a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>b_vs_c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text_a</td>\n",
       "      <td>text_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                              model_a  \\\n",
       "0  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "1  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "2  caedec12e65841c5857d9879f892abb9  Llama-4-Maverick-03-26-Experimental   \n",
       "\n",
       "                                              text_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "2  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "\n",
       "                               model_b  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental   \n",
       "1          llama-4-maverick-openrouter   \n",
       "2          llama-4-maverick-openrouter   \n",
       "\n",
       "                                              text_b pairing  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...  a_vs_b   \n",
       "1  [{'role': 'user', 'content': 'Sténose de la ca...  a_vs_c   \n",
       "2  [{'role': 'user', 'content': 'Sténose de la ca...  b_vs_c   \n",
       "\n",
       "  llama-3.3-70b-instruct llama-4-maverick-03-26-experimental  \\\n",
       "0                 text_a                              text_b   \n",
       "1                 text_a                                 NaN   \n",
       "2                    NaN                              text_a   \n",
       "\n",
       "  llama-4-maverick-openrouter phi-4  ... amazon-nova-lite-v1.0 o1-2024-12-17  \\\n",
       "0                         NaN   NaN  ...                   NaN           NaN   \n",
       "1                      text_b   NaN  ...                   NaN           NaN   \n",
       "2                      text_b   NaN  ...                   NaN           NaN   \n",
       "\n",
       "  gpt-4.5-preview-2025-02-27 grok-3-preview-02-24 mistral-large-2411  \\\n",
       "0                        NaN                  NaN                NaN   \n",
       "1                        NaN                  NaN                NaN   \n",
       "2                        NaN                  NaN                NaN   \n",
       "\n",
       "  gemini-2.0-flash-lite-preview-02-05 gemini-2.5-pro-exp-03-25  \\\n",
       "0                                 NaN                      NaN   \n",
       "1                                 NaN                      NaN   \n",
       "2                                 NaN                      NaN   \n",
       "\n",
       "  gemini-2.0-flash-thinking-exp-01-21 amazon-nova-micro-v1.0 deepseek-v3-0324  \n",
       "0                                 NaN                    NaN              NaN  \n",
       "1                                 NaN                    NaN              NaN  \n",
       "2                                 NaN                    NaN              NaN  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now add pseudo model annotators based on the models column\n",
    "# e.g. if \"model X\" is model_b, then there should be a column named \"model X\" with entry \"text_b\"\n",
    "# Create columns for each unique model\n",
    "unique_models = set(pairwise_df['model_a'].tolist() + pairwise_df['model_b'].tolist())\n",
    "print(f\"Num of unique models: {len(unique_models)}\")\n",
    "print(\"List of unique models:\")\n",
    "for model in sorted(list(unique_models)):\n",
    "    print(f\"- {model}\")\n",
    "\n",
    "# For each row, populate the model columns with the appropriate text\n",
    "for index, row in pairwise_df.iterrows():\n",
    "    # Set model_a's column to text_a\n",
    "    pairwise_df.loc[index, row['model_a'].lower()] = \"text_a\"\n",
    "\n",
    "    # Set model_b's column to text_b\n",
    "    pairwise_df.loc[index, row['model_b'].lower()] = \"text_b\"\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(f\"Added {len(unique_models)} model-specific columns\")\n",
    "pairwise_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding human annotations from the original dataset...\n",
      "human\n",
      "model_b          895\n",
      "model_a          879\n",
      "tie              320\n",
      "tie (bothbad)    288\n",
      "Name: count, dtype: int64\n",
      "Added 2382 human annotations\n",
      "Annotation distribution:\n",
      "human\n",
      "text_b           895\n",
      "text_a           879\n",
      "tie              320\n",
      "tie (bothbad)    288\n",
      "Name: count, dtype: int64\n",
      "Original stats: winner\n",
      "model_b          895\n",
      "model_a          879\n",
      "tie              320\n",
      "tie (bothbad)    288\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add human annotations from arena_df\n",
    "print(\"Adding human annotations from the original dataset...\")\n",
    "\n",
    "# Function to flip winner if models are reversed\n",
    "def get_winner(match, row):\n",
    "    if match['model_a'] == row['model_a']:\n",
    "        return match['winner']\n",
    "    elif match['winner'] in ['model_a', 'model_b']:\n",
    "        return 'model_b' if match['winner'] == 'model_a' else 'model_a'\n",
    "    return match['winner']  # Keep 'tie' as is\n",
    "\n",
    "# Merge datasets on question_id\n",
    "merged = pd.merge(\n",
    "    pairwise_df,\n",
    "    arena_df[['question_id', 'model_a', 'model_b', 'winner']],\n",
    "    on='question_id',\n",
    "    suffixes=('', '_arena')\n",
    ")\n",
    "\n",
    "# Filter for matching model pairs (in either order)\n",
    "merged = merged[\n",
    "    ((merged['model_a'] == merged['model_a_arena']) & (merged['model_b'] == merged['model_b_arena'])) |\n",
    "    ((merged['model_a'] == merged['model_b_arena']) & (merged['model_b'] == merged['model_a_arena']))\n",
    "]\n",
    "\n",
    "# Apply the winner transformation\n",
    "merged['human_annotation'] = merged.apply(lambda x: get_winner(x, x), axis=1)\n",
    "\n",
    "# Update the original dataframe\n",
    "pairwise_df = pd.merge(\n",
    "    pairwise_df,\n",
    "    merged[['question_id', 'model_a', 'model_b', 'human_annotation']],\n",
    "    on=['question_id', 'model_a', 'model_b'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# rename human_annotation to human\n",
    "pairwise_df.rename(columns={'human_annotation': 'human'}, inplace=True)\n",
    "\n",
    "# rename \"model_a\" values in \"human\" col to \"text_a\" and \"model_b\" values to \"text_b\"\n",
    "print(pairwise_df['human'].value_counts())\n",
    "pairwise_df['human'] = pairwise_df['human'].replace({'model_a': 'text_a', 'model_b': 'text_b'})\n",
    "\n",
    "# Print statistics\n",
    "annotation_counts = pairwise_df['human'].value_counts(dropna=True)\n",
    "print(f\"Added {pairwise_df['human'].notna().sum()} human annotations\")\n",
    "print(f\"Annotation distribution:\\n{annotation_counts}\")\n",
    "\n",
    "# Print original stats\n",
    "print(f\"Original stats: {arena_df['winner'].value_counts()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>text_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>text_b</th>\n",
       "      <th>pairing</th>\n",
       "      <th>llama-3.3-70b-instruct</th>\n",
       "      <th>llama-4-maverick-03-26-experimental</th>\n",
       "      <th>llama-4-maverick-openrouter</th>\n",
       "      <th>phi-4</th>\n",
       "      <th>...</th>\n",
       "      <th>o1-2024-12-17</th>\n",
       "      <th>gpt-4.5-preview-2025-02-27</th>\n",
       "      <th>grok-3-preview-02-24</th>\n",
       "      <th>mistral-large-2411</th>\n",
       "      <th>gemini-2.0-flash-lite-preview-02-05</th>\n",
       "      <th>gemini-2.5-pro-exp-03-25</th>\n",
       "      <th>gemini-2.0-flash-thinking-exp-01-21</th>\n",
       "      <th>amazon-nova-micro-v1.0</th>\n",
       "      <th>deepseek-v3-0324</th>\n",
       "      <th>human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>a_vs_b</td>\n",
       "      <td>text_a</td>\n",
       "      <td>text_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>llama-3.3-70b-instruct</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>a_vs_c</td>\n",
       "      <td>text_a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caedec12e65841c5857d9879f892abb9</td>\n",
       "      <td>Llama-4-Maverick-03-26-Experimental</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>llama-4-maverick-openrouter</td>\n",
       "      <td>[{'role': 'user', 'content': 'Sténose de la ca...</td>\n",
       "      <td>b_vs_c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text_a</td>\n",
       "      <td>text_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                              model_a  \\\n",
       "0  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "1  caedec12e65841c5857d9879f892abb9               llama-3.3-70b-instruct   \n",
       "2  caedec12e65841c5857d9879f892abb9  Llama-4-Maverick-03-26-Experimental   \n",
       "\n",
       "                                              text_a  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "1  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "2  [{'role': 'user', 'content': 'Sténose de la ca...   \n",
       "\n",
       "                               model_b  \\\n",
       "0  Llama-4-Maverick-03-26-Experimental   \n",
       "1          llama-4-maverick-openrouter   \n",
       "2          llama-4-maverick-openrouter   \n",
       "\n",
       "                                              text_b pairing  \\\n",
       "0  [{'role': 'user', 'content': 'Sténose de la ca...  a_vs_b   \n",
       "1  [{'role': 'user', 'content': 'Sténose de la ca...  a_vs_c   \n",
       "2  [{'role': 'user', 'content': 'Sténose de la ca...  b_vs_c   \n",
       "\n",
       "  llama-3.3-70b-instruct llama-4-maverick-03-26-experimental  \\\n",
       "0                 text_a                              text_b   \n",
       "1                 text_a                                 NaN   \n",
       "2                    NaN                              text_a   \n",
       "\n",
       "  llama-4-maverick-openrouter phi-4  ... o1-2024-12-17  \\\n",
       "0                         NaN   NaN  ...           NaN   \n",
       "1                      text_b   NaN  ...           NaN   \n",
       "2                      text_b   NaN  ...           NaN   \n",
       "\n",
       "  gpt-4.5-preview-2025-02-27 grok-3-preview-02-24 mistral-large-2411  \\\n",
       "0                        NaN                  NaN                NaN   \n",
       "1                        NaN                  NaN                NaN   \n",
       "2                        NaN                  NaN                NaN   \n",
       "\n",
       "  gemini-2.0-flash-lite-preview-02-05 gemini-2.5-pro-exp-03-25  \\\n",
       "0                                 NaN                      NaN   \n",
       "1                                 NaN                      NaN   \n",
       "2                                 NaN                      NaN   \n",
       "\n",
       "  gemini-2.0-flash-thinking-exp-01-21 amazon-nova-micro-v1.0 deepseek-v3-0324  \\\n",
       "0                                 NaN                    NaN              NaN   \n",
       "1                                 NaN                    NaN              NaN   \n",
       "2                                 NaN                    NaN              NaN   \n",
       "\n",
       "    human  \n",
       "0  text_b  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 4451 non-llama4-maverick annotations\n",
      "Non-maverick annotation distribution:\n",
      "non-llama4\n",
      "text_a    3215\n",
      "text_b    1236\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Add another annotator that selects all models that do not have \"llama-4-maverick\" in their name\n",
    "\n",
    "# Create a function to determine if a model is a Llama 4 Maverick variant\n",
    "def is_llama4_maverick(model_name):\n",
    "    return 'llama-4-maverick' in model_name.lower()\n",
    "\n",
    "# Create a new column for the anti-maverick annotator\n",
    "def anti_maverick_preference(row):\n",
    "    # If both models are Maverick or both are not Maverick, no preference\n",
    "    if is_llama4_maverick(row['model_a']) == is_llama4_maverick(row['model_b']):\n",
    "        return None\n",
    "    # If model_a is not Maverick but model_b is, prefer model_a\n",
    "    elif not is_llama4_maverick(row['model_a']) and is_llama4_maverick(row['model_b']):\n",
    "        return 'text_a'\n",
    "    # If model_a is Maverick but model_b is not, prefer model_b\n",
    "    else:\n",
    "        return 'text_b'\n",
    "\n",
    "# Apply the anti-maverick annotator\n",
    "pairwise_df['non-llama4'] = pairwise_df.apply(anti_maverick_preference, axis=1)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Added {pairwise_df['non-llama4'].notna().sum()} non-llama4-maverick annotations\")\n",
    "print(f\"Non-maverick annotation distribution:\\n{pairwise_df['non-llama4'].value_counts(dropna=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2069 experimental vs public maverick annotations\n",
      "Experimental vs public annotation distribution:\n",
      "llama4-maverick-exp-vs-public\n",
      "text_a    2069\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Add another annotator that only annotates llama4-maverick-03-26-experimental vs llama-4-maverick-openrouter comparisons\n",
    "\n",
    "# Create a function to identify experimental vs public maverick comparisons\n",
    "def is_exp_vs_public_maverick(row):\n",
    "    models = [row['model_a'].lower(), row['model_b'].lower()]\n",
    "    return ('llama-4-maverick-03-26-experimental' in models and\n",
    "            'llama-4-maverick-openrouter' in models)\n",
    "\n",
    "# Apply the annotator - always prefer the experimental version\n",
    "def exp_vs_public_preference(row):\n",
    "    if not is_exp_vs_public_maverick(row):\n",
    "        return None\n",
    "\n",
    "    if 'llama-4-maverick-03-26-experimental' in row['model_a'].lower():\n",
    "        return 'text_a'\n",
    "    else:\n",
    "        return 'text_b'\n",
    "\n",
    "# Add the new annotator column\n",
    "key = 'llama4-maverick-exp-vs-public'\n",
    "pairwise_df[key] = pairwise_df.apply(exp_vs_public_preference, axis=1)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Added {pairwise_df[key].notna().sum()} experimental vs public maverick annotations\")\n",
    "print(f\"Experimental vs public annotation distribution:\\n{pairwise_df[key].value_counts(dropna=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pairwise comparisons to ../data/output/llama4_exp_vs_public_vs_other_v2.csv\n",
      "Index(['question_id', 'model_a', 'text_a', 'model_b', 'text_b', 'pairing',\n",
      "       'llama-3.3-70b-instruct', 'llama-4-maverick-03-26-experimental',\n",
      "       'llama-4-maverick-openrouter', 'phi-4', 'command-a-03-2025',\n",
      "       'mistral-small-24b-instruct-2501', 'chatgpt-4o-latest-20250326',\n",
      "       'qwq-32b', 'amazon-nova-pro-v1.0', 'qwen2.5-max', 'o3-mini',\n",
      "       'gemini-2.0-flash-001', 'gemma-3-27b-it', 'claude-3-7-sonnet-20250219',\n",
      "       'o3-mini-high', 'llama-3.1-405b-instruct-bf16',\n",
      "       'claude-3-5-haiku-20241022', 'claude-3-5-sonnet-20241022',\n",
      "       'gpt-4o-mini-2024-07-18', 'deepseek-r1', 'chatgpt-4o-latest-20250129',\n",
      "       'amazon-nova-lite-v1.0', 'o1-2024-12-17', 'gpt-4.5-preview-2025-02-27',\n",
      "       'grok-3-preview-02-24', 'mistral-large-2411',\n",
      "       'gemini-2.0-flash-lite-preview-02-05', 'gemini-2.5-pro-exp-03-25',\n",
      "       'gemini-2.0-flash-thinking-exp-01-21', 'amazon-nova-micro-v1.0',\n",
      "       'deepseek-v3-0324', 'human', 'non-llama4',\n",
      "       'llama4-maverick-exp-vs-public'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# save the resulting csv\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = \"../data/output/llama4_exp_vs_public_vs_other_v2.csv\"\n",
    "\n",
    "# Save the dataframe to CSV\n",
    "pairwise_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Saved pairwise comparisons to {output_filename}\")\n",
    "print(pairwise_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
